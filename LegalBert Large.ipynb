{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c73f2b3d-668e-496c-9ccf-6f67f1050f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline, AutoModel\n",
    "import json\n",
    "import textwrap\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain import PromptTemplate,  LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import pandas as pd\n",
    "import time\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477e071e-2366-43d9-b78c-45dbdbdacdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HF_AUTH_TOKEN\")\n",
    "login(token=HUGGINGFACEHUB_API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a694dfa3-4e8d-4707-a1a5-385e68b12b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"pile-of-law/legalbert-large-1.7M-2\")\n",
    "# model = AutoModel.from_pretrained(\"pile-of-law/legalbert-large-1.7M-2\")\n",
    "# device = torch.device(\"cuda\")\n",
    "# model.to(device)  # Move the model to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0897358a-41fe-468a-986b-ecf4614f0bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save models and tokenizer locally\n",
    "# model.save_pretrained('./legal-bert-large')\n",
    "# tokenizer.save_pretrained('./legal-bert-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e14315c-de73-47fe-bc18-d7cd67af2945",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = \"./legal-bert-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "model = AutoModel.from_pretrained(model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb6f474c-68db-4cfd-b423-4327fbc7eb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a28d4bfe-900a-48f8-b229-59e242d6cbfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(32000, 1024, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 1024)\n",
       "    (token_type_embeddings): Embedding(2, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1dfff9a-caac-4e33-aa3e-fb94b79640ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the majority opinion from the CaseLaw json file\n",
    "def load_and_extract_data(file_path):\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    for o in data[\"casebody\"][\"data\"][\"opinions\"]:\n",
    "        if o[\"type\"] == \"majority\":\n",
    "            return o[\"text\"]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "# Grab chunks of text to summarize\n",
    "# It has an overlap to make sure each chunk has context of the previous chuck\n",
    "def chunk_text_with_overlap(text, chunk_word_count, overlap_word_count):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    index = 0\n",
    "\n",
    "    while index < len(words):\n",
    "        current_chunk_end = index + chunk_word_count\n",
    "        current_chunk_end = min(current_chunk_end, len(words))\n",
    "        chunk = \" \".join(words[index:current_chunk_end])\n",
    "        chunks.append(chunk)\n",
    "\n",
    "        index += chunk_word_count - overlap_word_count\n",
    "\n",
    "        # force it to advance to avoid an infinite loop\n",
    "        if index >= current_chunk_end:\n",
    "            index = current_chunk_end\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Sort of overkill on saving a summary to a text file\n",
    "# Some naming logic and error checking added in\n",
    "def save_summary_to_text(summary, output_folder, file_path, condensed=False):\n",
    "    \n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    if condensed:\n",
    "        summary_file_name = f\"{base_name}_condensed_summary.txt\"\n",
    "    else:\n",
    "        summary_file_name = f\"{base_name}_summary.txt\"\n",
    "    \n",
    "    summary_file_path = os.path.join(output_folder, summary_file_name)\n",
    "\n",
    "    try:\n",
    "        with open(summary_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(summary)\n",
    "        print(f\"Summary successfully written to {summary_file_name}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Unable to write to file: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "566e0b5f-4ac4-4c2d-88e0-ec73571b06f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates cosine similarity for sentence embeddings\n",
    "# Pair sentences with scores, then sorts in descending order\n",
    "# Pick top 'num_sentence' number of sentences\n",
    "# START: REFACTOR FROM <https://towardsdatascience.com/extractive-summarization-using-bert-966e912f4142 and https://www.analyticsvidhya.com/blog/2023/03/exploring-the-extractive-method-of-text-summarization/>\n",
    "# Also used GPT-4 in debugging\n",
    "def extractive_summarization(text, num_sentences):\n",
    "\n",
    "    # Use NLTK's sentence tokenizer to split the text into individual sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    tokenized_sentences = [tokenizer.encode(sent, add_special_tokens=True) for sent in sentences]\n",
    "    \n",
    "    max_len = 0\n",
    "    for i in tokenized_sentences:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "    \n",
    "    padded_sentences = []\n",
    "    for i in tokenized_sentences:\n",
    "        while len(i) < max_len:\n",
    "            i.append(0)\n",
    "        padded_sentences.append(i)\n",
    "        \n",
    "    input_ids = torch.tensor(padded_sentences)\n",
    "    \n",
    "    attention_mask = [[float(i != 0.0) for i in seq] for seq in padded_sentences]\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    \n",
    "    input_ids = input_ids.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids, attention_mask=attention_mask)[0]\n",
    "    \n",
    "    sentence_embeddings = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence_embeddings.append(torch.mean(last_hidden_states[i], dim=0).cpu().numpy())\n",
    "        \n",
    "    similarity_matrix = cosine_similarity(sentence_embeddings)\n",
    "    \n",
    "    sentence_scores = [sum(similarity_matrix[i]) for i in range(len(sentences))]\n",
    "    \n",
    "    sentence_score_pairs = list(enumerate(sentence_scores))\n",
    "    \n",
    "    sorted_sentences = sorted(sentence_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    summary_sentences = [sentences[index] for index, _ in sorted_sentences[:num_sentences]]\n",
    "    \n",
    "    summary = ' '.join(summary_sentences)\n",
    "    \n",
    "    return summary\n",
    "# END: REFACTOR FROM <https://towardsdatascience.com/extractive-summarization-using-bert-966e912f4142 and https://www.analyticsvidhya.com/blog/2023/03/exploring-the-extractive-method-of-text-summarization/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81ef0ffb-e2df-4616-b512-0b450fca38a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d739552e-eb21-4910-997b-b2e4df00e45d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73425660-4662-4414-a8a1-9cee7556439c",
   "metadata": {},
   "source": [
    "# Bulk Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e44cc9c7-04f2-4fa1-b56a-e61ea151dca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizing a case document and using a csv file to keep track of\n",
    "# This code has been refactored seeveral times so the name and batch_size are kind of outdated\n",
    "def summarize_a_batch_of_case_documents(batch_size, processed_files_csv):\n",
    "    start = time.time()\n",
    "    if os.path.exists(processed_files_csv):\n",
    "        processed_files_df = pd.read_csv(processed_files_csv)\n",
    "    else:\n",
    "        processed_files_df = pd.DataFrame(columns=[\"file_name\", \"time_elapsed\"])\n",
    "    processed_count = 0\n",
    "    filenames = sorted(os.listdir(input_folder))\n",
    "    \n",
    "    # Start from where the last entry left off\n",
    "    last_processed_index = 0\n",
    "    if not processed_files_df.empty:\n",
    "        last_filename = processed_files_df['file_name'].iloc[-1]\n",
    "        last_processed_index = filenames.index(last_filename) + 1\n",
    "\n",
    "    # Process files starting from the last processed one\n",
    "    for filename in filenames[last_processed_index:]:\n",
    "        if processed_count >= batch_size:\n",
    "            break\n",
    "        print(\"Processing: \", filename)\n",
    "        if filename.endswith(\".json\"):\n",
    "            if filename in processed_files_df['file_name'].values:\n",
    "                    print(f\"File {filename} has already been processed. Skipping.\")\n",
    "                    continue\n",
    "            else:\n",
    "                try:\n",
    "                    summarize_a_case_document(filename)\n",
    "                except:\n",
    "                    print(f\"Error processing {filename}\")\n",
    "                processed_count += 1\n",
    "                end = time.time()\n",
    "                new_row = {\"file_name\": filename,\n",
    "                           \"time_elapsed\": end - start\n",
    "                          }\n",
    "                processed_files_df = pd.concat([processed_files_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "                processed_files_df.to_csv(processed_files_csv, index=False)\n",
    "\n",
    "# This code summarizes a case when given a json file\n",
    "# It gets the major opinion, chucks it, summarizes it, then save it as an individual txt file\n",
    "def summarize_a_case_document(filename):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    opinion = load_and_extract_data(file_path)\n",
    "    opinion = str(opinion)\n",
    "    \n",
    "    chunk_word_count = 1000\n",
    "    overlap_word_count = 200\n",
    "    \n",
    "    chunks = chunk_text_with_overlap(opinion, chunk_word_count, overlap_word_count)\n",
    "\n",
    "    chunk_summaries = summarize_chunks(chunks, chunk_word_count)\n",
    "    final_summary = ' '.join(chunk_summaries)\n",
    "    \n",
    "    save_summary_to_text(final_summary, output_folder, file_path, condensed=False)\n",
    "\n",
    "# It's here that we decided the number of sentences to use in extractive summarization\n",
    "# we picked between 2 and 7 sentences\n",
    "# given that chunk in roughly 1000 words, we would add a sentence every 100 words\n",
    "def summarize_chunks(chunks, chunk_word_count):\n",
    "    min_sentences = 2\n",
    "    standard_summary_length = 10\n",
    "    max_sentences = 7\n",
    "\n",
    "    summaries = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        chunk_length = len(chunk.split())\n",
    "        proportional_sentences_number = int((chunk_length / chunk_word_count) * standard_summary_length)\n",
    "        \n",
    "        sentences_to_summarize = max(proportional_sentences_number, min_sentences)\n",
    "\n",
    "        sentences_to_summarize = min(max_sentences, proportional_sentences_number)\n",
    "        \n",
    "        # Perform extractive summarization on the chunk using 'sentences_to_summarize' as the number of sentences to include in the summary\n",
    "        summary = extractive_summarization(chunk, sentences_to_summarize)\n",
    "        summaries.append(summary)\n",
    "        \n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5da279-aa17-402d-a140-0e1708386bf5",
   "metadata": {},
   "source": [
    "## Getting the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8142555-c1c7-4510-8b63-8d5064910563",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'ref_case_jsons_train'\n",
    "output_folder = 'ref_case_txt_train'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "processed_files_df = None\n",
    "processed_files_csv = 'processed_files_for_legalBertLarge_train_set.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28311222-e766-4db5-a326-f69d5995a97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(os.listdir(input_folder))):\n",
    "for i in range(10000):\n",
    "    summarize_a_batch_of_case_documents(1, processed_files_csv)\n",
    "    if i % 500 == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426e2a84-b58d-4709-8abb-caedaac2b6c5",
   "metadata": {},
   "source": [
    "## Getting the val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37060aac-0b1a-41d6-b2a9-1a5597b54ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'ref_case_jsons_val'\n",
    "output_folder = 'ref_case_txt_val'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "processed_files_df = None\n",
    "processed_files_csv = 'processed_files_for_legalBertLarge_val_set.csv'\n",
    "for i in range(len(os.listdir(input_folder))):\n",
    "    print(i)\n",
    "    summarize_a_batch_of_case_documents(1, processed_files_csv)\n",
    "    if i % 500 == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7033d92f-7789-4eec-a75b-30ef34756ff2",
   "metadata": {},
   "source": [
    "## Getting the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef0d3e0-8214-40a9-aac3-e7cd07120987",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'ref_case_jsons_test'\n",
    "output_folder = 'ref_case_txt_test'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "processed_files_df = None\n",
    "processed_files_csv = 'processed_files_for_legalBertLarge_test_set.csv'\n",
    "for i in range(len(os.listdir(input_folder))):\n",
    "    print(i)\n",
    "    summarize_a_batch_of_case_documents(1, processed_files_csv)\n",
    "    if i % 500 == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118f0468-f7f7-4189-803a-70f24a794626",
   "metadata": {},
   "source": [
    "# Some tests below to see if LegalBert-Large can help condense generated holdings\n",
    "## It's not very reliable or at least I disagree with the sentences it picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "513247ac-2592-4bc4-b79d-8ebb31f18f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "holding = \"\"\"\n",
    "The holding in this case is that the presumption of legitimacy should not be utilized to perpetuate a falsehood if the truth can be discovered. The court found that the mother's husband had been a substantial presence in the child's life and desires to continue to exercise parental rights, and therefore, the need for joining him as a party whose interests \"might be inequitably affected by\" the resulting order of filiation is manifest. The court also found that the results of a human leucocyte antigen test showed a 99.53% probability that the petitioner is the child's father, which was sufficient to overcome the presumption of legitimacy that arises when a child is born to a married woman. However, the court noted that if William refuses to submit to a blood test, an adverse inference may then be drawn against him.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b1808e7d-38ab-4e90-a5f1-333f07aeabc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The court also found that the results of a human leucocyte antigen test showed a 99.53% probability that the petitioner is the child\\'s father, which was sufficient to overcome the presumption of legitimacy that arises when a child is born to a married woman. The court found that the mother\\'s husband had been a substantial presence in the child\\'s life and desires to continue to exercise parental rights, and therefore, the need for joining him as a party whose interests \"might be inequitably affected by\" the resulting order of filiation is manifest.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractive_summarization(holding, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a2da01cd-56da-4b4c-a2a7-515f308372fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "holding = \"\"\"\n",
    "The holding of the case is:  \"Where a mother's husband has been a substantial presence in the\n",
    "child's life and desires to continue to exercise parental rights, the need for joining him as a\n",
    "party whose interests'might be inequitably affected by' the resulting order of filiation is\n",
    "manifest, and the court may order joinder on its own motion.\"  In other words, the court has the\n",
    "discretion to join a person as a party to a paternity proceeding if their interests may be impacted\n",
    "by the outcome of the case, even if they are not a party to the proceeding. This holding is based on\n",
    "the idea that the truth about paternity should be discovered and that the presumption of legitimacy\n",
    "should not be used to perpetuate a falsehood.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "163748e9-9cba-4df8-a9df-8821126aac31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe holding of the case is:  \"Where a mother\\'s husband has been a substantial presence in the\\nchild\\'s life and desires to continue to exercise parental rights, the need for joining him as a\\nparty whose interests\\'might be inequitably affected by\\' the resulting order of filiation is\\nmanifest, and the court may order joinder on its own motion.\" In other words, the court has the\\ndiscretion to join a person as a party to a paternity proceeding if their interests may be impacted\\nby the outcome of the case, even if they are not a party to the proceeding.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractive_summarization(holding, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0ca6ad-3b4c-45a7-bc4c-df5fa4ebc9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
