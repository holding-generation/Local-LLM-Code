{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ffb1dd-f870-4379-8eef-88ddc792541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "import json\n",
    "import textwrap\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain import PromptTemplate,  LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b0979e-d803-43ae-b372-df5dd72bc355",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HF_AUTH_TOKEN\")\n",
    "login(token=HUGGINGFACEHUB_API_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856b142f-8dd1-4235-9c05-efc69bd8da35",
   "metadata": {},
   "source": [
    "# This notebook is a combination the various models used to generate holdings and some summaries\n",
    "## - To use this notebook, roll to the section you want to use and only run those cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7095cb30-54d8-4b74-9f54-02a0b2d9df16",
   "metadata": {},
   "source": [
    "# Skip the next two cells if you don't want to use Llama 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59adc15-db39-4fbd-a8d2-486cb44056d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = \"./Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_directory,\n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             # load_in_8bit=True,\n",
    "                                             load_in_4bit=True\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a579b9e-84e1-424a-8774-251060a33fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 1024,\n",
    "                do_sample=True,\n",
    "                top_k=30,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1531e1d-6199-4eac-b758-56047f7ec343",
   "metadata": {},
   "source": [
    "# The cell below has important functions\n",
    "## It's best to run it, regardless of which task and model you're using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe50231-e60c-4233-b32e-15e729c042c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
    "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "    return prompt_template\n",
    "\n",
    "def cut_off_text(text, prompt):\n",
    "    cutoff_phrase = prompt\n",
    "    index = text.find(cutoff_phrase)\n",
    "    if index != -1:\n",
    "        return text[:index]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_substring(string, substring):\n",
    "    return string.replace(substring, \"\")\n",
    "\n",
    "\n",
    "\n",
    "def generate(text):\n",
    "    prompt = get_prompt(text)\n",
    "    with torch.autocast('cuda', dtype=torch.bfloat16):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "        outputs = model.generate(**inputs,\n",
    "                                 max_new_tokens=1024,\n",
    "                                 eos_token_id=tokenizer.eos_token_id,\n",
    "                                 pad_token_id=tokenizer.eos_token_id,\n",
    "                                 )\n",
    "        final_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        final_outputs = cut_off_text(final_outputs, '</s>')\n",
    "        final_outputs = remove_substring(final_outputs, prompt)\n",
    "\n",
    "    return final_outputs#, outputs\n",
    "\n",
    "def parse_text(text):\n",
    "        wrapped_text = textwrap.fill(text, width=100)\n",
    "        print(wrapped_text +'\\n\\n')\n",
    "        # return assistant_text\n",
    "\n",
    "def count_words(input_string):\n",
    "    words = input_string.split(\" \")\n",
    "    return len(words)\n",
    "\n",
    "def summarize_chunks(chunks, model, tokenizer):\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        output = llm_chain.run(chunk)\n",
    "        # print(count_words(output))\n",
    "        # parse_text(output)\n",
    "        summaries.append(output)\n",
    "    return summaries\n",
    "\n",
    "def create_final_summary(summaries):\n",
    "    # Option 1: Just join the summaries\n",
    "    final_summary = ' '.join(summaries)\n",
    "\n",
    "    # Option 2: Apply another round of summarization (can be useful for coherence)\n",
    "    # final_summary = generate(final_summary)  # This is recursive and might degrade quality\n",
    "\n",
    "    return final_summary\n",
    "\n",
    "def chunk_text_with_overlap(text, chunk_word_count, overlap_word_count):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    index = 0\n",
    "\n",
    "    while index < len(words):\n",
    "        # Calculate the end index for the current chunk\n",
    "        current_chunk_end = index + chunk_word_count\n",
    "        \n",
    "        # We don't want to overshoot the list of words for the current chunk\n",
    "        current_chunk_end = min(current_chunk_end, len(words))\n",
    "\n",
    "        # Create the chunk\n",
    "        chunk = \" \".join(words[index:current_chunk_end])\n",
    "        chunks.append(chunk)\n",
    "\n",
    "        # Calculate the start index for the next chunk (considering overlap)\n",
    "        index += chunk_word_count - overlap_word_count\n",
    "\n",
    "        # If the calculated index doesn't advance (due to large overlap), we force it to advance to avoid an infinite loop\n",
    "        if index >= current_chunk_end:\n",
    "            index = current_chunk_end\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Function to load data from the JSON file and extract the desired information.\n",
    "def load_and_extract_data(file_path):\n",
    "    # Reading the file.\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)  # Parsing the JSON data.\n",
    "\n",
    "    for o in data[\"casebody\"][\"data\"][\"opinions\"]:\n",
    "        if o[\"type\"] == \"majority\":\n",
    "            return o[\"text\"]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def save_summary_to_text(summary, output_folder, file_path, condensed=False):\n",
    "    \"\"\"\n",
    "    Save the content of 'summary' to a text file derived from the name of the input file.\n",
    "    \"\"\"\n",
    "    # Extract the base file name without extension\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "    # Construct the new file name for the summary\n",
    "    if condensed:\n",
    "        summary_file_name = f\"{base_name}_condensed_summary.txt\"\n",
    "    else:\n",
    "        summary_file_name = f\"{base_name}_summary.txt\"\n",
    "    \n",
    "    # Construct the full path for the summary file\n",
    "    summary_file_path = os.path.join(output_folder, summary_file_name)\n",
    "\n",
    "    try:\n",
    "        # Using 'with' for proper file closure\n",
    "        with open(summary_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(summary)\n",
    "        print(f\"Summary successfully written to {summary_file_name}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Unable to write to file: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def read_file(file_path):\n",
    "    \"\"\"\n",
    "    Read the content of a text file.\n",
    "\n",
    "    :param file_path: str, path to the file to read.\n",
    "    :return: str, content of the file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        return content\n",
    "    except IOError as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab072408-6e04-4556-9026-30e9dca50209",
   "metadata": {},
   "source": [
    "# Bulk Opinion Summarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd11c45-118d-4857-922a-5a7b4fd8c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
    "instruction = \"Summarize the following case document. Remember to include the relevant facts and rules of the case. {text}\"\n",
    "system_prompt = \"You are a legal expert who specializes in summarizing case documents while including the legal facts.\"\n",
    "\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812c93ac-f8c2-46ac-8e6f-0f823bcbf93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'ref_case_small'\n",
    "output_folder = 'ref_case_summaries_small'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "processed_files_df = None\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
    "instruction = \"Summarize the following case document. Remember to include the relevant facts and rules of the case. {text}\"\n",
    "system_prompt = \"You are a legal expert who specializes in summarizing case documents while including the legal facts.\"\n",
    "\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "def summarize_a_batch_of_case_documents(batch_size):\n",
    "    # Define the filename for the processed files DataFrame\n",
    "    processed_files_csv = 'processed_files_for_summarizing_small.csv'\n",
    "    \n",
    "    # Check if the CSV file exists and load it, otherwise create an empty DataFrame\n",
    "    if os.path.exists(processed_files_csv):\n",
    "        processed_files_df = pd.read_csv(processed_files_csv)\n",
    "    else:\n",
    "        processed_files_df = pd.DataFrame(columns=[\"file_name\"])\n",
    "    \n",
    "    # Counter for the number of processed files in the batch\n",
    "    processed_count = 0\n",
    "\n",
    "    # Retrieve all filenames, sorted to ensure consistency across runs\n",
    "    filenames = sorted(os.listdir(input_folder))\n",
    "    \n",
    "    # Start from where the last entry left off\n",
    "    last_processed_index = 0\n",
    "    if not processed_files_df.empty:\n",
    "        last_filename = processed_files_df['file_name'].iloc[-1]\n",
    "        last_processed_index = filenames.index(last_filename) + 1\n",
    "\n",
    "    # Process files starting from the last processed one\n",
    "    for filename in filenames[last_processed_index:]:\n",
    "        if processed_count >= batch_size:\n",
    "            break\n",
    "        print(\"Processing: \", filename)\n",
    "        if filename.endswith(\".json\"):\n",
    "            if filename in processed_files_df['file_name'].values:\n",
    "                    print(f\"File {filename} has already been processed. Skipping.\")\n",
    "                    continue\n",
    "            else:\n",
    "                summarize_a_case_document(filename)\n",
    "                processed_count += 1  # Increment the processed files counter\n",
    "\n",
    "                # Add the file to the processed DataFrame\n",
    "                new_row = {\"file_name\": filename}\n",
    "                processed_files_df = pd.concat([processed_files_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "                processed_files_df.to_csv(processed_files_csv, index=False)\n",
    "                \n",
    "def summarize_a_case_document(filename):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    opinion = load_and_extract_data(file_path)\n",
    "    opinion = str(opinion)\n",
    "    \n",
    "    chunk_word_count = 1000  # for instance, around 1000 words per chunk\n",
    "    overlap_word_count = 200  # for instance, overlap of 200 words\n",
    "    \n",
    "    chunks = chunk_text_with_overlap(opinion, chunk_word_count, overlap_word_count)\n",
    "\n",
    "    chunk_summaries = summarize_chunks(chunks, model, tokenizer)\n",
    "    final_summary = create_final_summary(chunk_summaries)\n",
    "    \n",
    "    # Save the summary to a text file\n",
    "    save_summary_to_text(final_summary, output_folder, file_path, condensed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2855513d-8248-48eb-8c3b-30d6943684a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(16):\n",
    "    start = time.time()\n",
    "    summarize_a_batch_of_case_documents(1)\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed:\", end - start)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07d1a0e-2af8-4040-b205-3519c18457e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba358405-55aa-47e6-bc03-5edd90af020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10500f2c-c025-48ea-bf9c-b25e59e6d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3715298b-39c6-46ca-b275-49033e8933c7",
   "metadata": {},
   "source": [
    "## Bulk Summarization without special prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a658beb-719b-4e61-a699-e6c4b9d4608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_a_batch_of_case_documents(batch_size, processed_files_csv):\n",
    "    # Check if the CSV file exists and load it, otherwise create an empty DataFrame\n",
    "    if os.path.exists(processed_files_csv):\n",
    "        processed_files_df = pd.read_csv(processed_files_csv)\n",
    "    else:\n",
    "        processed_files_df = pd.DataFrame(columns=[\"file_name\"])\n",
    "    \n",
    "    # Counter for the number of processed files in the batch\n",
    "    processed_count = 0\n",
    "\n",
    "    # Retrieve all filenames, sorted to ensure consistency across runs\n",
    "    filenames = sorted(os.listdir(input_folder))\n",
    "    \n",
    "    # Start from where the last entry left off\n",
    "    last_processed_index = 0\n",
    "    if not processed_files_df.empty:\n",
    "        last_filename = processed_files_df['file_name'].iloc[-1]\n",
    "        last_processed_index = filenames.index(last_filename) + 1\n",
    "\n",
    "    # Process files starting from the last processed one\n",
    "    for filename in filenames[last_processed_index:]:\n",
    "        if processed_count >= batch_size:\n",
    "            break\n",
    "        print(\"Processing: \", filename)\n",
    "        if filename.endswith(\".json\"):\n",
    "            if filename in processed_files_df['file_name'].values:\n",
    "                    print(f\"File {filename} has already been processed. Skipping.\")\n",
    "                    continue\n",
    "            else:\n",
    "                summarize_a_case_document(filename)\n",
    "                processed_count += 1  # Increment the processed files counter\n",
    "\n",
    "                # Add the file to the processed DataFrame\n",
    "                new_row = {\"file_name\": filename}\n",
    "                processed_files_df = pd.concat([processed_files_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "                processed_files_df.to_csv(processed_files_csv, index=False)\n",
    "                \n",
    "def summarize_a_case_document(filename):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    opinion = load_and_extract_data(file_path)\n",
    "    opinion = str(opinion)\n",
    "    \n",
    "    chunk_word_count = 1000  # for instance, around 1000 words per chunk\n",
    "    overlap_word_count = 200  # for instance, overlap of 200 words\n",
    "    \n",
    "    chunks = chunk_text_with_overlap(opinion, chunk_word_count, overlap_word_count)\n",
    "\n",
    "    chunk_summaries = summarize_chunks(chunks, model, tokenizer)\n",
    "    final_summary = create_final_summary(chunk_summaries)\n",
    "    \n",
    "    # Save the summary to a text file\n",
    "    save_summary_to_text(final_summary, output_folder, file_path, condensed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890faeaf-4e0d-4431-b64e-df1d58e7d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
    "instruction = \"Summarize the following: {text}\"\n",
    "system_prompt = \"You are an expert summarizier.\"\n",
    "\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f932752-0e84-4345-8285-0f90e7ab6748",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'ref_case_small'\n",
    "output_folder = 'ref_case_llama_regular_summaries_small'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "processed_files_df = None\n",
    "# Define the filename for the processed files DataFrame\n",
    "processed_files_csv = 'processed_files_for_llama_regular_summarizing_small.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f500acfa-d1cb-48a6-a763-24180fb0f568",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(os.listdir(input_folder))):\n",
    "    start = time.time()\n",
    "    summarize_a_batch_of_case_documents(1, processed_files_csv)\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed:\", end - start)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace1e305-8dd1-44ae-8492-de42ce50e235",
   "metadata": {},
   "source": [
    "# Bulk Holding Generation with Llama 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7330935f-f197-49a0-bec5-3076f1476639",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
    "instruction = \"Use the case document to extract the concise holding. {text} Here is a hint on the holding: {hint}\"\n",
    "system_prompt = \"You are a legal expert who specializes in extracting accurate and concise holdings from case documents.\"\n",
    "\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "print(template)\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\", \"hint\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1252d5-024f-44ee-ab57-9b5e765dff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_df = pd.read_csv('case_references.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e3b34f-4d06-4afb-b833-258b600cbc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_holding(filename):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    summary = read_file(file_path)\n",
    "    current_case_summary = str(summary)\n",
    "\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    case_number = int(base_name.split(\"_\")[1])\n",
    "\n",
    "    holding_hint = reference_df.iloc[case_number - 1, 6]\n",
    "    print(holding_hint)\n",
    "\n",
    "    input_dict = {'text': current_case_summary,\n",
    "                  'hint': holding_hint\n",
    "                 }\n",
    "    \n",
    "    output = llm_chain.run(input_dict)\n",
    "\n",
    "    holding_file_name = f\"{base_name}_holding.txt\"\n",
    "    \n",
    "    # Construct the full path for the summary file\n",
    "    holding_file_path = os.path.join(output_folder, holding_file_name)\n",
    "\n",
    "    try:\n",
    "        # Using 'with' for proper file closure\n",
    "        with open(holding_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(output)\n",
    "        print(f\"Summary successfully written to {holding_file_path}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Unable to write to file: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def batch_process_holdings(batch_size, processed_files_csv):\n",
    "    \n",
    "    # Check if the CSV file exists and load it, otherwise create an empty DataFrame\n",
    "    if os.path.exists(processed_files_csv):\n",
    "        processed_files_df = pd.read_csv(processed_files_csv)\n",
    "    else:\n",
    "        processed_files_df = pd.DataFrame(columns=[\"file_name\"])\n",
    "    \n",
    "    # Counter for the number of processed files in the batch\n",
    "    processed_count = 0\n",
    "\n",
    "    # Retrieve all filenames, sorted to ensure consistency across runs\n",
    "    filenames = sorted(os.listdir(input_folder))\n",
    "    \n",
    "    # Start from where the last entry left off\n",
    "    last_processed_index = 0\n",
    "    if not processed_files_df.empty:\n",
    "        last_filename = processed_files_df['file_name'].iloc[-1]\n",
    "        last_processed_index = filenames.index(last_filename) + 1\n",
    "\n",
    "    # Process files starting from the last processed one\n",
    "    for filename in filenames[last_processed_index:]:\n",
    "        if processed_count >= batch_size:\n",
    "            break\n",
    "        print(\"Processing: \", filename)\n",
    "        if filename in processed_files_df['file_name'].values:\n",
    "                print(f\"File {filename} has already been processed. Skipping.\")\n",
    "                continue\n",
    "        else:\n",
    "            generate_holding(filename)\n",
    "            processed_count += 1  # Increment the processed files counter\n",
    "\n",
    "            # Add the file to the processed DataFrame\n",
    "            new_row = {\"file_name\": filename}\n",
    "            processed_files_df = pd.concat([processed_files_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            processed_files_df.to_csv(processed_files_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09299f02-53c8-4930-8751-b1a41eb5c7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'ref_case_llama_regular_summaries_small'\n",
    "output_folder = 'ref_case_llama_regular_sum_llama_holdings_small'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "processed_files_df = None\n",
    "# Define the filename for the processed files DataFrame\n",
    "processed_files_csv = 'processed_files_llama_regular_sum_llama_for_holdings_small.csv'\n",
    "\n",
    "for i in range(len(os.listdir(input_folder))):\n",
    "    start = time.time()\n",
    "    batch_process_holdings(1,processed_files_csv)\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed:\", end - start)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af6a9f7-0f4b-4f72-ae30-03f745738ea2",
   "metadata": {},
   "source": [
    "# Bulk Holding Generation Without Hint \n",
    "## - Using Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1b2025-c56f-4aa1-9b3d-7213ed645391",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
    "instruction = \"Use the case document to extract the concise holding. {text}\"\n",
    "system_prompt = \"You are a legal expert who specializes in extracting accurate and concise holdings from case documents.\"\n",
    "\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "print(template)\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6fe027-6ac8-4ada-853a-63cb2b6d07f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_df = pd.read_csv('case_references.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b8033b-0fe7-4416-ace3-5ad15f6daf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_holding(filename):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    summary = read_file(file_path)\n",
    "    current_case_summary = str(summary)\n",
    "\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    case_number = int(base_name.split(\"_\")[1])\n",
    "\n",
    "    holding_hint = reference_df.iloc[case_number - 1, 6]\n",
    "    print(holding_hint)\n",
    "\n",
    "    input_dict = {'text': current_case_summary,\n",
    "                  'hint': holding_hint\n",
    "                 }\n",
    "    \n",
    "    output = llm_chain.run(input_dict)\n",
    "\n",
    "    holding_file_name = f\"{base_name}_holding.txt\"\n",
    "    \n",
    "    # Construct the full path for the summary file\n",
    "    holding_file_path = os.path.join(output_folder, holding_file_name)\n",
    "\n",
    "    try:\n",
    "        # Using 'with' for proper file closure\n",
    "        with open(holding_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(output)\n",
    "        print(f\"Summary successfully written to {holding_file_path}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Unable to write to file: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def batch_process_holdings(batch_size, processed_files_csv):\n",
    "    \n",
    "    # Check if the CSV file exists and load it, otherwise create an empty DataFrame\n",
    "    if os.path.exists(processed_files_csv):\n",
    "        processed_files_df = pd.read_csv(processed_files_csv)\n",
    "    else:\n",
    "        processed_files_df = pd.DataFrame(columns=[\"file_name\"])\n",
    "    \n",
    "    # Counter for the number of processed files in the batch\n",
    "    processed_count = 0\n",
    "\n",
    "    # Retrieve all filenames, sorted to ensure consistency across runs\n",
    "    filenames = sorted(os.listdir(input_folder))\n",
    "    \n",
    "    # Start from where the last entry left off\n",
    "    last_processed_index = 0\n",
    "    if not processed_files_df.empty:\n",
    "        last_filename = processed_files_df['file_name'].iloc[-1]\n",
    "        last_processed_index = filenames.index(last_filename) + 1\n",
    "\n",
    "    # Process files starting from the last processed one\n",
    "    for filename in filenames[last_processed_index:]:\n",
    "        if processed_count >= batch_size:\n",
    "            break\n",
    "        print(\"Processing: \", filename)\n",
    "        if filename in processed_files_df['file_name'].values:\n",
    "                print(f\"File {filename} has already been processed. Skipping.\")\n",
    "                continue\n",
    "        else:\n",
    "            generate_holding(filename)\n",
    "            processed_count += 1  # Increment the processed files counter\n",
    "\n",
    "            # Add the file to the processed DataFrame\n",
    "            new_row = {\"file_name\": filename}\n",
    "            processed_files_df = pd.concat([processed_files_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            processed_files_df.to_csv(processed_files_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a154bb-aa36-4c6d-b64e-82099ab34f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'ref_case_legalBertLarge_summaries_small'\n",
    "output_folder = 'ref_case_legalBertLarge_sum_llama_holdings_without_hint_small'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "processed_files_df = None\n",
    "# Define the filename for the processed files DataFrame\n",
    "processed_files_csv = 'processed_files_legalBertLarge_sum_llama_for_holdings_without_hint_small.csv'\n",
    "\n",
    "for i in range(len(os.listdir(input_folder))):\n",
    "    start = time.time()\n",
    "    batch_process_holdings(1,processed_files_csv)\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed:\", end - start)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c84f64-5703-4e37-986f-02117fe58715",
   "metadata": {},
   "source": [
    "# Bulk Holding Generation with Mistral\n",
    "## Will use between 16 and 21 Gb of GPU RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0980c365-ad66-48ac-9923-271b6a010772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "torch.set_default_device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae45533-218d-4227-aa8b-6e645bd2ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = \"./Mistral-7B-OpenOrca\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_directory,\n",
    "                                             torch_dtype=\"auto\",\n",
    "                                             device_map='auto',\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a19a9ee-0f23-4fcc-ba99-81f3559bfbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 1024,\n",
    "                do_sample=True,\n",
    "                top_k=30,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id = 3200\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f6dd6f-8345-44b6-97e7-8c2a952e308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(text):\n",
    "    # print(\"prompt: \", prompt)\n",
    "    # prompt = get_prompt(text)\n",
    "    # print(\"prompt: \", prompt)\n",
    "    with torch.autocast('cuda', dtype=torch.bfloat16):\n",
    "        # inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "        device = 'cuda'\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "        outputs = model.generate(**model_inputs,\n",
    "                                 max_new_tokens=1024,\n",
    "                                 eos_token_id=tokenizer.eos_token_id,\n",
    "                                 # pad_token_id=tokenizer.eos_token_id,\n",
    "                                 pad_token_id = 3200\n",
    "                                 )\n",
    "        final_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        \n",
    "        # final_outputs = cut_off_text(final_outputs, '</s>')\n",
    "        # final_outputs = remove_substring(final_outputs, prompt)\n",
    "\n",
    "    return final_outputs#, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c103b4-fc0c-4d53-8717-d5af7863bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"<|im_start|>system\\n\n",
    "You are MistralOrca, a large language model trained by Alignment Lab AI. Write out your reasoning step-by-step to be sure you get the right answers!\\n\n",
    "<|im_end|>\\n\n",
    "<|im_start|>user\\n\n",
    "what is the meaning of life?\\n\n",
    "<|im_end|>\"\"\"\n",
    "\n",
    "encodeds = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "import textwrap\n",
    "\n",
    "def wrap_text(text, width=90): #preserve_newlines\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "def generate_holding(filename, reference_df, instruction, system_prompt, max_length, hint_bool):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    summary = read_file(file_path)\n",
    "    current_case_summary = str(summary)\n",
    "    # print(\"\\ncurrent_case_summary: \", current_case_summary)\n",
    "\n",
    "    # Check and truncate current_case_summary if needed\n",
    "    tokens = tokenizer(current_case_summary, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    while tokens.input_ids.size(1) > max_length - 500:\n",
    "        print(\"\\n\\n\\nTruncation\")\n",
    "\n",
    "        # Truncate the text from the back\n",
    "        current_case_summary = \" \".join(current_case_summary.split(' ')[:-1])\n",
    "        # tokens = tokenizer(current_case_summary, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    case_number = int(base_name.split(\"_\")[1])\n",
    "\n",
    "    holding_hint = reference_df.iloc[case_number - 1, 6]\n",
    "\n",
    "    if hint_bool is True:\n",
    "        input_dict = {'text': current_case_summary,\n",
    "                      'hint': holding_hint\n",
    "                     }\n",
    "    else:\n",
    "        # instruction = instruction.format(text=current_case_summary)\n",
    "        input_dict = {'text': current_case_summary,\n",
    "                      'hint': holding_hint\n",
    "                     }\n",
    "    output = llm_chain.run(input_dict)\n",
    "    print(\"output: \", output)\n",
    "\n",
    "    holding_file_name = f\"{base_name}_holding.txt\"\n",
    "    \n",
    "    # Construct the full path for the summary file\n",
    "    holding_file_path = os.path.join(output_folder, holding_file_name)\n",
    "\n",
    "    try:\n",
    "        # Using 'with' for proper file closure\n",
    "        with open(holding_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(output)\n",
    "        print(f\"Summary successfully written to {holding_file_path}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Unable to write to file: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def batch_process_holdings(batch_size, processed_files_csv, reference_df, instruction, system_prompt, max_length, hint_bool=False):\n",
    "    \n",
    "    # Check if the CSV file exists and load it, otherwise create an empty DataFrame\n",
    "    if os.path.exists(processed_files_csv):\n",
    "        processed_files_df = pd.read_csv(processed_files_csv)\n",
    "    else:\n",
    "        processed_files_df = pd.DataFrame(columns=[\"file_name\"])\n",
    "    \n",
    "    # Counter for the number of processed files in the batch\n",
    "    processed_count = 0\n",
    "\n",
    "    # Retrieve all filenames, sorted to ensure consistency across runs\n",
    "    filenames = sorted(os.listdir(input_folder))\n",
    "    \n",
    "    # Start from where the last entry left off\n",
    "    last_processed_index = 0\n",
    "    if not processed_files_df.empty:\n",
    "        last_filename = processed_files_df['file_name'].iloc[-1]\n",
    "        last_processed_index = filenames.index(last_filename) + 1\n",
    "\n",
    "    # Process files starting from the last processed one\n",
    "    for filename in filenames[last_processed_index:]:\n",
    "        if processed_count >= batch_size:\n",
    "            break\n",
    "        print(\"Processing: \", filename)\n",
    "        if filename in processed_files_df['file_name'].values:\n",
    "                print(f\"File {filename} has already been processed. Skipping.\")\n",
    "                continue\n",
    "        else:\n",
    "            generate_holding(filename, \n",
    "                             reference_df = reference_df, \n",
    "                             instruction = instruction,\n",
    "                             system_prompt = system_prompt,\n",
    "                             max_length = max_length,\n",
    "                             hint_bool = hint_bool\n",
    "                            )\n",
    "            processed_count += 1  # Increment the processed files counter\n",
    "\n",
    "            # Add the file to the processed DataFrame\n",
    "            new_row = {\"file_name\": filename}\n",
    "            processed_files_df = pd.concat([processed_files_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            processed_files_df.to_csv(processed_files_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ce4923-d34f-48cd-99c7-aefbcd5cf4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_summary = \"French Lilly further testified that some time thereafter the policy written by Mrs. Foster was mailed to him, inasmuch as plaintiff also resided in Wyoming County, and that he thereupon mailed the policy to the plaintiff, not at that time being aware of the fact that the policy was written for the same man who had been refused insurance previously by the witness. As a result of the telephone conversation, the plaintiff called upon Mrs. Foster, who issued the insurance policy in question. Later, however, the home office learned of the false answer contained in the application; and on March 8, 1957, wrote a letter to the plaintiff detailing the facts in relation to the false answer contained in the application, and notifying the plaintiff that his insurance policy was rescinded. At the time the letter of December 20 was written, the home office did not know of the accident involving the cow, or of the occurrence which resulted in the destruction of the automobile, and did not know of the fact that the application made by plaintiff contained a false statement of facts. French Lilly of Oceana, in Wyoming County, an agent for the defendant company, testified that “in the neighborhood of July in 1956”, the plaintiff showed to the witness a letter disclosing that the plaintiff’s automobile insurance with another company was being cancelled and that thereupon the witness refused to write a policy of insurance on behalf of the defendant company covering plaintiff’s automobile. As a consequence thereof, the home office wrote a letter to the plaintiff, dated December 20, 1956, three days after the automobile was demolished, notifying the plaintiff that his insurance policy would be cancelled as of January 2, 1957, and advising him to obtain proper insurance in the meantime with another company. The defendant offered the letter of March 8 for introduction in evidence, the plaintiff objected, and the court refused to permit the introduction of such letter as a part of the evidence to be considered by the jury. The defendant offered the letter of March 8 for introduction in evidence, the plaintiff objected, and the court refused to permit the introduction of such letter as a part of the evidence to be considered by the jury. Thereupon, the court made the following statement to the jury: “Ladies and gentlemen of the jury, there are two questions to be decided in this case. After the completion of the testimony at the trial before a jury, the defendant made a motion for a directed verdict in its favor, which motion was overruled. In his written opinion, which was made a part of the record, the eminent trial judge stated: “I am of the opinion that the knowledge of the agent of the company at Oceana is imputed to its principal, the defendant company, and the defendant company having had knowledge of a prior cancellation through its agent and in its principal offices, had an election to either treat the policy as void or rescind it as of a future date. The company elected to rescind and prior to the date of rescinding, the loss occurred.” Prior to the time of the trial, the defendant filed its specification of defense in accordance with the provisions of Code, 56-4-21, a portion of which was as follows: “That the plaintiff fraudulently procured the said policy of insurance by the wilful, intentional, making of false and fraudulent answers and misrepresentations upon his application for said policy of insurance, with full knowledge at the time of making said false statements and answers that the defendant would not have issued said policy of insurance had the defendant known that the statements and answers as given were false. The trial court deducted this sum from $2,800.00, the value placed on the automobile by the jury’s verdict, and entered judgment for the plaintiff for the balance, amounting to $271.45. The defendant, however, in order to save the point, took testimony in relation to the letter outside the presence and hearing of the jury, and the letter was made a part of the record. 2 syl., 140 S. E. 61; Kincaid v. Equitable Life Assur. 45 C.J.S., Insurance, Section 605, page 438. SchWarzbach v. Ohio Valley Protective Union, 25 W. Va. 622, syl. “The insurer is not precluded from setting up the falsity of answers in the application where it appears that insured knew at the time they were being written or before signing the application that they were written falsely in order to defraud the company.” 45 C.J.S., Insurance, Section 732, page 741. It has been held that, under such a state of facts, “the policy is thereby forfeited.” Saltesz v. The Sovereign Camp of the Woodmen of the World, 110 W. Va. 513, syl., 159 S. E. 513. “Plaintiff cannot, at the trial or in the appellate court, rely on an estoppel not set forth in his reply.” Capehart v. Mutual Benefit Health and Accident Ass’n., 111 W. Va. 317, syl., 161 S. E. 609. Health & Accident v. Ratcliffe, 163 Va. 325, 175 S. E. 870, 874. It has been held that, under such circumstances, “the policy will ordinarily be forfeited.” Faulkiner v. Equitable Life Insurance Co., 144, W. Va. 193, syl., 107 S. E. 2d 360. It has been held that, under such a state of facts, “the policy is thereby forfeited.” Saltesz v. The Sovereign Camp of the Woodmen of the World, 110 W. Va. 513, syl., 159 S. E. 513. Although in some jurisdictions it is held that a contract of insurance procured by fraud is void, as a general rule, such a contract is voidable at the option of insurer on discovery of the fraud.” 45 C.J.S., Insurance, Section 473(2), page 152.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beb0c3a-bca7-4af6-94dc-7f7eac8b84ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Use the case document to extract the concise holding. {text}\"\n",
    "# instruction = instruction.format(text=case_summary)\n",
    "system_prompt=\"You are MistralOrca, a legal expert who specializes in extracting accurate and concise holdings from case documents. Write out your short and succinct!\"\n",
    "# output = generate(instruction,\n",
    "#                   system_prompt=system_prompt,\n",
    "#                   max_length=4096\n",
    "#                   )\n",
    "# print(\"output: \", output)\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
    "\n",
    "\n",
    "chat = [\n",
    "  {\"role\": \"system\", \"content\": system_prompt},\n",
    "  {\"role\": \"user\", \"content\": instruction}\n",
    "]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(chat, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "prompt = PromptTemplate(template=input_text, input_variables=[\"text\"])\n",
    "print(\"prompt: \", prompt)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "output = llm_chain.run(case_summary)\n",
    "print(\"output: \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f87f31-27ec-4ae0-89a5-5803f7f2330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'ref_case_legalBertLarge_summaries_small'\n",
    "output_folder = 'ref_case_legalBertLarge_sum_Mistral_holdings_without_hint_small'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "processed_files_df = None\n",
    "reference_df = pd.read_csv('case_references.csv')\n",
    "processed_files_csv = 'processed_files_legalBertLarge_sum_for_Mistral_holdings_without_hint_small.csv'\n",
    "max_length = 4096\n",
    "instruction = \"Use the case document to extract the concise holding. {text}\"\n",
    "system_prompt=\"You are MistralOrca, a legal expert who specializes in extracting accurate and concise holdings from case documents. Write out your short and succinct!\",\n",
    "\n",
    "for i in range(len(os.listdir(input_folder))):\n",
    "# for i in range(1):\n",
    "    start = time.time()\n",
    "    batch_process_holdings(1,processed_files_csv, reference_df, instruction, system_prompt, max_length, hint_bool=False)\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed:\", end - start)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b0b266-7c02-4db1-9655-d6b742da8b37",
   "metadata": {},
   "source": [
    "# Bulk Holding Generation with Mistral Using Hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18e80e-e7b1-4232-a5ec-6a8343d2fd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'ref_case_legalBertLarge_summaries_small'\n",
    "output_folder = 'ref_case_legalBertLarge_sum_Mistral_holdings_with_hint_small'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "processed_files_df = None\n",
    "reference_df = pd.read_csv('case_references.csv')\n",
    "processed_files_csv = 'processed_files_legalBertLarge_sum_for_Mistral_holdings_with_hint_small.csv'\n",
    "max_length = 4096\n",
    "instruction = \"Use the case document to extract the concise holding. {text} Here is a hint on the holding: {hint}\"\n",
    "system_prompt=\"You are MistralOrca, a legal expert who specializes in extracting accurate and concise holdings from case documents. Write out your short and succinct!\",\n",
    "\n",
    "for i in range(len(os.listdir(input_folder))):\n",
    "# for i in range(1):\n",
    "    start = time.time()\n",
    "    batch_process_holdings(1,processed_files_csv, reference_df, instruction, system_prompt, max_length, hint_bool=True)\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed:\", end - start)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d48fe9-9353-4d41-8adf-11f9f0ccba23",
   "metadata": {},
   "source": [
    "# Loading and running the partially fine-tuned LLama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc6fd34-a965-49ca-a39d-0d7b1c013391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a280a6-c91a-4057-bc36-d5d0c31a1166",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f39b570-2ba9-4d3f-a7bc-3896e2f1e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde97042-b679-4c04-8caf-4e8ec27e083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Llama 2 7B, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    token=HUGGINGFACEHUB_API_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f81e9fe-a15f-4fd4-817d-44088d126f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True, token=HUGGINGFACEHUB_API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b9dcd5-3e22-4cd1-a5e8-868810dab59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed to update to the most recent version of peft\n",
    "# pip install -U git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1d76ae-297a-478a-93f3-414d21c82d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"checkpoint-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5858dbcf-44ab-4b78-b7fd-5f9b6be65bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\",\n",
    "                model=ft_model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 1024,\n",
    "                do_sample=True,\n",
    "                top_k=30,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea29dfb-9be9-4cc7-a330-279514c5f85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
    "instruction = \"Use the case document to extract the concise holding. {text}\"\n",
    "system_prompt = \"You are a legal expert who specializes in extracting accurate and concise holdings from case documents.\"\n",
    "\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "print(template)\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1adfaca-42e0-46e0-a227-b33dd0783ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_summary = \"Defendant’s attorney then told the jury that Lonnie Smith, a welding instructor, would testify that defendant discussed with him plans to make a lamp stand using the shotgun, and that defendant agreed to sell the shotgun to Robert Sandoval, an undercover agent, because the gun was worthless except as a part of defendant’s welding project. In opening statements the jury heard defendant’s version of how he came into possession of the shotgun and his theory of defense. After he repeated that he did not view it as a firearm, the district court sustained the government’s objection. Defendant then presented the testimony of defendant’s counsel in a related state court case, who testified that the firearm was inoperable at the state preliminary hearing. The government then presented testimony by Glenn Alexander, the detective who received the gun from Sandoval and logged it into the evidence room. The district court then sustained the government’s objection to defense counsel’s further questioning about whether the gun worked. Defendant took the stand and testified he concluded that the gun was not a firearm because it was broken, and he stored it as junk. After he repeated that he did not view it as a firearm, the district court sustained the government’s objection. United States v. Yannott, 42 F.3d 999, 1006 (6th Cir.1994) (gun inoperable due to broken firing pin still designed or readily converted to expel projectile so still firearm under § 922), cert. Defendant’s proposed jury instructions state an “ignorance of the law” defense, a defense which is “easily rejected.” United States v. Capps, 77 F.3d 350, 353 (10th Cir.) On redirect defendant testified that he did not believe it was a firearm because he thought a firearm had “to be able to do bodily harm to a person.” Id. The district court properly excluded the “good faith” jury instruction suggesting that ignorance of the law was a defense, and the corresponding evidence. Lonnie Smith testified that defendant had planned to use the old gun as a lamp stand. § 5861(d), based on defendant’s possession of a machine gun, required that government prove defendant knew of the features of his gun that brought it -within the scope of that act). Defendant did not challenge the constitutionality of the statute. The district court did not allow defense counsel to introduce defendant’s drawings concerning the design of the table with the shotgun. Also, when the government produced evidence that the gun could be repaired (e.g., “easily converted”) to expel a projectile, see IV R. 91-93 (Sandoval); V R. 143-48 (Alexander), defendant was allowed to cross-examine Sandoval before the government's objection to his question to Alexander on the issue was sustained. In Staples the Court determined that this knowledge requirement meant that the government had to prove that the defendant knew his weapon could shoot or be readily restored to shooting automatically. Further, even the proffered testimony of the defense’s investigator indicated he was able to make the gun work. Defendant asserts the court erroneously excluded evidence that he did not think the gun could expel a projectile or readily be converted to do so. United States v. Yannott, 42 F.3d 999, 1006 (6th Cir.1994) (gun inoperable due to broken firing pin still designed or readily converted to expel projectile so still firearm under § 922), cert.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62619e03-ed35-4118-ab11-a38c35e591e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm_chain.run(case_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77576d74-273c-44f9-bad5-70edde6e7b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6b1c5f-0f46-4dd2-ade6-a4677bf0fc31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49beacdb-a8b2-4c01-aa05-cae6ca42df13",
   "metadata": {},
   "source": [
    "# Longformer Bulk Summarization\n",
    "## currently does not chunk text larger than context window\n",
    "## Will modify in the future if this proves to be a viable route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329a8437-1f1a-41d2-a7a6-6fee46ac63c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LEDForConditionalGeneration, LEDTokenizer, pipeline, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d92a848-230a-425a-8d68-d8de180e8a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LEDForConditionalGeneration.from_pretrained('allenai/led-base-16384')\n",
    "tokenizer = LEDTokenizer.from_pretrained(\"allenai/led-base-16384\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fc82b0-6a28-4081-9cf7-10e00a7d597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LED_model_folder = \"./LED\"\n",
    "# os.makedirs(LED_model_folder, exist_ok=True)\n",
    "# model.save_pretrained(LED_model_folder)\n",
    "# tokenizer.save_pretrained(LED_model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c75739-b742-43a9-ba7f-15c6aca02b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LED_model_folder = \"./LED\"\n",
    "# tokenizer = LEDForConditionalGeneration.from_pretrained(LED_model_folder)\n",
    "# model = LEDTokenizer.from_pretrained(LED_model_folder)\n",
    "LED_summarizer_pipeline = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=0)  # 'device=0' to use the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342ff9bf-0fd5-4960-8937-bcf7e0e98b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_a_batch_of_case_documents(batch_size, processed_files_csv):\n",
    " \n",
    "    # Check if the CSV file exists and load it, otherwise create an empty DataFrame\n",
    "    if os.path.exists(processed_files_csv):\n",
    "        processed_files_df = pd.read_csv(processed_files_csv)\n",
    "    else:\n",
    "        processed_files_df = pd.DataFrame(columns=[\"file_name\"])\n",
    "    \n",
    "    # Counter for the number of processed files in the batch\n",
    "    processed_count = 0\n",
    "\n",
    "    # Retrieve all filenames, sorted to ensure consistency across runs\n",
    "    filenames = sorted(os.listdir(input_folder))\n",
    "    \n",
    "    # Start from where the last entry left off\n",
    "    last_processed_index = 0\n",
    "    if not processed_files_df.empty:\n",
    "        last_filename = processed_files_df['file_name'].iloc[-1]\n",
    "        last_processed_index = filenames.index(last_filename) + 1\n",
    "\n",
    "    # Process files starting from the last processed one\n",
    "    for filename in filenames[last_processed_index:]:\n",
    "        if processed_count >= batch_size:\n",
    "            break\n",
    "        print(\"Processing: \", filename)\n",
    "        if filename.endswith(\".json\"):\n",
    "            if filename in processed_files_df['file_name'].values:\n",
    "                    print(f\"File {filename} has already been processed. Skipping.\")\n",
    "                    continue\n",
    "            else:\n",
    "                summarize_a_case_document(filename)\n",
    "                processed_count += 1  # Increment the processed files counter\n",
    "\n",
    "                # Add the file to the processed DataFrame\n",
    "                new_row = {\"file_name\": filename}\n",
    "                processed_files_df = pd.concat([processed_files_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "                processed_files_df.to_csv(processed_files_csv, index=False)\n",
    "                \n",
    "def summarize_a_case_document(filename):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    opinion = load_and_extract_data(file_path)\n",
    "    opinion = str(opinion)\n",
    "    \n",
    "    # chunk_word_count = 1000  # for instance, around 1000 words per chunk\n",
    "    # overlap_word_count = 200  # for instance, overlap of 200 words\n",
    "    \n",
    "    # chunks = chunk_text_with_overlap(opinion, chunk_word_count, overlap_word_count)\n",
    "\n",
    "    summary = LED_summarizer_pipeline(opinion, min_length=30, max_length=1000)\n",
    "    summary_text = summary[0]['summary_text']\n",
    "\n",
    "    # Save the summary to a text file\n",
    "    save_summary_to_text(summary_text, output_folder, file_path, condensed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d55caf4-e87b-4fbb-9e42-ece03dcf1923",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'ref_case_small'\n",
    "output_folder = 'ref_case_LED_summaries_small'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "processed_files_df = None\n",
    "\n",
    "# Define the filename for the processed files DataFrame\n",
    "processed_files_csv = 'processed_files_for_LED_summarizing_small.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df10321-6a59-4378-a84d-fe3a32be182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(os.listdir(input_folder))):\n",
    "    start = time.time()\n",
    "    summarize_a_batch_of_case_documents(1, processed_files_csv)\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed:\", end - start)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956b84eb-7b2a-4abd-baf6-7361c11ab4e3",
   "metadata": {},
   "source": [
    "# Long T5 Bulk Summarization\n",
    "## Currently does not chunk text larger than context window\n",
    "## Will modify in the future if this proves to be a viable route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80965a0c-dae9-4f3f-86cf-e06e0f3de8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, LongT5ForConditionalGeneration, pipeline\n",
    "model = (\n",
    "    LongT5ForConditionalGeneration.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")\n",
    "    .to(\"cuda\")\n",
    "    .half()\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62e00cd-1354-4652-9ee8-1524eb1d0cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Long_T5_summarizer_pipeline = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=0)  # 'device=0' to use the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b970be-6a12-4319-9569-716f73ec633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_a_batch_of_case_documents(batch_size, processed_files_csv, pipeline):\n",
    " \n",
    "    # Check if the CSV file exists and load it, otherwise create an empty DataFrame\n",
    "    if os.path.exists(processed_files_csv):\n",
    "        processed_files_df = pd.read_csv(processed_files_csv)\n",
    "    else:\n",
    "        processed_files_df = pd.DataFrame(columns=[\"file_name\"])\n",
    "    \n",
    "    # Counter for the number of processed files in the batch\n",
    "    processed_count = 0\n",
    "\n",
    "    # Retrieve all filenames, sorted to ensure consistency across runs\n",
    "    filenames = sorted(os.listdir(input_folder))\n",
    "    \n",
    "    # Start from where the last entry left off\n",
    "    last_processed_index = 0\n",
    "    if not processed_files_df.empty:\n",
    "        last_filename = processed_files_df['file_name'].iloc[-1]\n",
    "        last_processed_index = filenames.index(last_filename) + 1\n",
    "\n",
    "    # Process files starting from the last processed one\n",
    "    for filename in filenames[last_processed_index:]:\n",
    "        if processed_count >= batch_size:\n",
    "            break\n",
    "        print(\"Processing: \", filename)\n",
    "        if filename.endswith(\".json\"):\n",
    "            if filename in processed_files_df['file_name'].values:\n",
    "                    print(f\"File {filename} has already been processed. Skipping.\")\n",
    "                    continue\n",
    "            else:\n",
    "                summarize_a_case_document(filename, pipeline)\n",
    "                processed_count += 1  # Increment the processed files counter\n",
    "\n",
    "                # Add the file to the processed DataFrame\n",
    "                new_row = {\"file_name\": filename}\n",
    "                processed_files_df = pd.concat([processed_files_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "                processed_files_df.to_csv(processed_files_csv, index=False)\n",
    "                \n",
    "def summarize_a_case_document(filename, pipeline):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    opinion = load_and_extract_data(file_path)\n",
    "    opinion = str(opinion)\n",
    "    \n",
    "    # chunk_word_count = 1000  # for instance, around 1000 words per chunk\n",
    "    # overlap_word_count = 200  # for instance, overlap of 200 words\n",
    "    \n",
    "    # chunks = chunk_text_with_overlap(opinion, chunk_word_count, overlap_word_count)\n",
    "\n",
    "    summary = pipeline(opinion, min_length=30, max_length=1000)\n",
    "    summary_text = summary[0]['summary_text']\n",
    "\n",
    "    # Save the summary to a text file\n",
    "    save_summary_to_text(summary_text, output_folder, file_path, condensed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c3b20b-46b7-4819-9df9-110bb8e952dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'ref_case_small'\n",
    "output_folder = 'ref_case_Long_T5_summaries_small'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "processed_files_df = None\n",
    "\n",
    "# Define the filename for the processed files DataFrame\n",
    "processed_files_csv = 'processed_files_for_Long_T5_summarizing_small.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b6213e-8442-4e01-9c0b-b644b18d0bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(os.listdir(input_folder))):\n",
    "    start = time.time()\n",
    "    summarize_a_batch_of_case_documents(1, processed_files_csv, Long_T5_summarizer_pipeline)\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed:\", end - start)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f7ffb0-a8f4-4a0f-9121-0e89b7b0831e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
