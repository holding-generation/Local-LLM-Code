{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ffb1dd-f870-4379-8eef-88ddc792541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "import json\n",
    "import textwrap\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain import PromptTemplate,  LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b0979e-d803-43ae-b372-df5dd72bc355",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HF_AUTH_TOKEN\")\n",
    "login(token=HUGGINGFACEHUB_API_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856b142f-8dd1-4235-9c05-efc69bd8da35",
   "metadata": {},
   "source": [
    "# This notebook is a combination the various models used to generate holdings and some summaries\n",
    "## - To use this notebook, roll to the section you want to use and only run those cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7095cb30-54d8-4b74-9f54-02a0b2d9df16",
   "metadata": {},
   "source": [
    "# Skip the next two cells if you don't want to use Llama 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59adc15-db39-4fbd-a8d2-486cb44056d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = \"./Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_directory,\n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             # load_in_8bit=True,\n",
    "                                             load_in_4bit=True\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a579b9e-84e1-424a-8774-251060a33fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# START: REFACTORED FROM <https://colab.research.google.com/drive/1Ssg-fffeJ0LG0m3DoTofeLPvOUQyG1h3?usp=sharing>\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 1024,\n",
    "                do_sample=True,\n",
    "                top_k=30,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "# END: REFACTORED FROM <https://colab.research.google.com/drive/1Ssg-fffeJ0LG0m3DoTofeLPvOUQyG1h3?usp=sharing>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1531e1d-6199-4eac-b758-56047f7ec343",
   "metadata": {},
   "source": [
    "# The cell below has important functions\n",
    "## It's best to run it, regardless of which task and model you're using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe50231-e60c-4233-b32e-15e729c042c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# START: COPIED FROM <https://colab.research.google.com/drive/1Ssg-fffeJ0LG0m3DoTofeLPvOUQyG1h3?usp=sharing>\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
    "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "    return prompt_template\n",
    "# END: COPIED FROM <https://colab.research.google.com/drive/1Ssg-fffeJ0LG0m3DoTofeLPvOUQyG1h3?usp=sharing>\n",
    "\n",
    "def count_words(input_string):\n",
    "    words = input_string.split(\" \")\n",
    "    return len(words)\n",
    "\n",
    "def summarize_chunks(chunks, model, tokenizer):\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        output = llm_chain.run(chunk)\n",
    "        summaries.append(output)\n",
    "    return summaries\n",
    "\n",
    "def create_final_summary(summaries):\n",
    "    # Option 1: Just join the summaries\n",
    "    final_summary = ' '.join(summaries)\n",
    "\n",
    "    # Option 2: Apply another round of summarization (We found this didn't work for holding factuality)\n",
    "    # final_summary = generate(final_summary)\n",
    "\n",
    "    return final_summary\n",
    "\n",
    "# Grab chunks of text to summarize\n",
    "# It has an overlap to make sure each chunk has context of the previous chuck\n",
    "def chunk_text_with_overlap(text, chunk_word_count, overlap_word_count):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    index = 0\n",
    "\n",
    "    while index < len(words):\n",
    "        current_chunk_end = index + chunk_word_count\n",
    "        current_chunk_end = min(current_chunk_end, len(words))\n",
    "        chunk = \" \".join(words[index:current_chunk_end])\n",
    "        chunks.append(chunk)\n",
    "\n",
    "        index += chunk_word_count - overlap_word_count\n",
    "\n",
    "        # force it to advance to avoid an infinite loop\n",
    "        if index >= current_chunk_end:\n",
    "            index = current_chunk_end\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Getting the majority opinion from the CaseLaw json file\n",
    "def load_and_extract_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    for o in data[\"casebody\"][\"data\"][\"opinions\"]:\n",
    "        if o[\"type\"] == \"majority\":\n",
    "            return o[\"text\"]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "# Sort of overkill on saving a summary to a text file\n",
    "# Some naming logic and error checking added in\n",
    "def save_summary_to_text(summary, output_folder, file_path, condensed=False):\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "    if condensed:\n",
    "        summary_file_name = f\"{base_name}_condensed_summary.txt\"\n",
    "    else:\n",
    "        summary_file_name = f\"{base_name}_summary.txt\"\n",
    "    \n",
    "    summary_file_path = os.path.join(output_folder, summary_file_name)\n",
    "\n",
    "    try:\n",
    "        with open(summary_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(summary)\n",
    "        print(f\"Summary successfully written to {summary_file_name}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Unable to write to file: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def read_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        return content\n",
    "    except IOError as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab072408-6e04-4556-9026-30e9dca50209",
   "metadata": {},
   "source": [
    "# Bulk Opinion Summarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd11c45-118d-4857-922a-5a7b4fd8c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
    "instruction = \"Summarize the following case document. Remember to include the relevant facts and rules of the case. {text}\"\n",
    "system_prompt = \"You are a legal expert who specializes in summarizing case documents while including the legal facts.\"\n",
    "\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812c93ac-f8c2-46ac-8e6f-0f823bcbf93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'ref_case_small'\n",
    "output_folder = 'ref_case_summaries_small'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "processed_files_df = None\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
    "instruction = \"Summarize the following case document. Remember to include the relevant facts and rules of the case. {text}\"\n",
    "system_prompt = \"You are a legal expert who specializes in summarizing case documents while including the legal facts.\"\n",
    "\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# Summarizing a case document and using a csv file to keep track of\n",
    "# This code has been refactored seeveral times so the name and batch_size are kind of outdated\n",
    "def summarize_a_batch_of_case_documents(batch_size):\n",
    "    processed_files_csv = 'processed_files_for_summarizing_small.csv'\n",
    "\n",
    "    if os.path.exists(processed_files_csv):\n",
    "        processed_files_df = pd.read_csv(processed_files_csv)\n",
    "    else:\n",
    "        processed_files_df = pd.DataFrame(columns=[\"file_name\"])\n",
    "    \n",
    "    processed_count = 0\n",
    "    filenames = sorted(os.listdir(input_folder))\n",
    "    \n",
    "    # Start from where the last entry left off\n",
    "    last_processed_index = 0\n",
    "    if not processed_files_df.empty:\n",
    "        last_filename = processed_files_df['file_name'].iloc[-1]\n",
    "        last_processed_index = filenames.index(last_filename) + 1\n",
    "\n",
    "    # Process files starting from the last processed one\n",
    "    for filename in filenames[last_processed_index:]:\n",
    "        if processed_count >= batch_size:\n",
    "            break\n",
    "        print(\"Processing: \", filename)\n",
    "        if filename.endswith(\".json\"):\n",
    "            if filename in processed_files_df['file_name'].values:\n",
    "                    print(f\"File {filename} has already been processed. Skipping.\")\n",
    "                    continue\n",
    "            else:\n",
    "                summarize_a_case_document(filename)\n",
    "                processed_count += 1\n",
    "\n",
    "                new_row = {\"file_name\": filename}\n",
    "                processed_files_df = pd.concat([processed_files_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "                processed_files_df.to_csv(processed_files_csv, index=False)\n",
    "                \n",
    "def summarize_a_case_document(filename):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    opinion = load_and_extract_data(file_path)\n",
    "    opinion = str(opinion)\n",
    "    \n",
    "    chunk_word_count = 1000\n",
    "    overlap_word_count = 200\n",
    "    \n",
    "    chunks = chunk_text_with_overlap(opinion, chunk_word_count, overlap_word_count)\n",
    "\n",
    "    chunk_summaries = summarize_chunks(chunks, model, tokenizer)\n",
    "    final_summary = create_final_summary(chunk_summaries)\n",
    "    \n",
    "    # Save the summary to a text file\n",
    "    save_summary_to_text(final_summary, output_folder, file_path, condensed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2855513d-8248-48eb-8c3b-30d6943684a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(16):\n",
    "    start = time.time()\n",
    "    summarize_a_batch_of_case_documents(1)\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed:\", end - start)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07d1a0e-2af8-4040-b205-3519c18457e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba358405-55aa-47e6-bc03-5edd90af020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10500f2c-c025-48ea-bf9c-b25e59e6d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3715298b-39c6-46ca-b275-49033e8933c7",
   "metadata": {},
   "source": [
    "## Bulk Summarization without special prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a658beb-719b-4e61-a699-e6c4b9d4608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizing a case document and using a csv file to keep track of\n",
    "# This code has been refactored seeveral times so the name and batch_size are kind of outdated\n",
    "def summarize_a_batch_of_case_documents(batch_size, processed_files_csv):\n",
    "    if os.path.exists(processed_files_csv):\n",
    "        processed_files_df = pd.read_csv(processed_files_csv)\n",
    "    else:\n",
    "        processed_files_df = pd.DataFrame(columns=[\"file_name\"])\n",
    "    \n",
    "    processed_count = 0\n",
    "    filenames = sorted(os.listdir(input_folder))\n",
    "    \n",
    "    # Start from where the last entry left off\n",
    "    last_processed_index = 0\n",
    "    if not processed_files_df.empty:\n",
    "        last_filename = processed_files_df['file_name'].iloc[-1]\n",
    "        last_processed_index = filenames.index(last_filename) + 1\n",
    "\n",
    "    # Process files starting from the last processed one\n",
    "    for filename in filenames[last_processed_index:]:\n",
    "        if processed_count >= batch_size:\n",
    "            break\n",
    "        print(\"Processing: \", filename)\n",
    "        if filename.endswith(\".json\"):\n",
    "            if filename in processed_files_df['file_name'].values:\n",
    "                    print(f\"File {filename} has already been processed. Skipping.\")\n",
    "                    continue\n",
    "            else:\n",
    "                summarize_a_case_document(filename)\n",
    "                processed_count += 1 \n",
    "\n",
    "                new_row = {\"file_name\": filename}\n",
    "                processed_files_df = pd.concat([processed_files_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "                processed_files_df.to_csv(processed_files_csv, index=False)\n",
    "                \n",
    "def summarize_a_case_document(filename):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    opinion = load_and_extract_data(file_path)\n",
    "    opinion = str(opinion)\n",
    "    \n",
    "    chunk_word_count = 1000\n",
    "    overlap_word_count = 200\n",
    "    \n",
    "    chunks = chunk_text_with_overlap(opinion, chunk_word_count, overlap_word_count)\n",
    "\n",
    "    chunk_summaries = summarize_chunks(chunks, model, tokenizer)\n",
    "    final_summary = create_final_summary(chunk_summaries)\n",
    "    \n",
    "    save_summary_to_text(final_summary, output_folder, file_path, condensed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890faeaf-4e0d-4431-b64e-df1d58e7d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
    "instruction = \"Summarize the following: {text}\"\n",
    "system_prompt = \"You are an expert summarizier.\"\n",
    "\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f932752-0e84-4345-8285-0f90e7ab6748",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'ref_case_small'\n",
    "output_folder = 'ref_case_llama_regular_summaries_small'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "processed_files_df = None\n",
    "processed_files_csv = 'processed_files_for_llama_regular_summarizing_small.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f500acfa-d1cb-48a6-a763-24180fb0f568",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(os.listdir(input_folder))):\n",
    "    start = time.time()\n",
    "    summarize_a_batch_of_case_documents(1, processed_files_csv)\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed:\", end - start)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace1e305-8dd1-44ae-8492-de42ce50e235",
   "metadata": {},
   "source": [
    "# Bulk Holding Generation with Llama 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7330935f-f197-49a0-bec5-3076f1476639",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
    "instruction = \"Use the case document to extract the concise holding. {text} Here is a hint on the holding: {hint}\"\n",
    "system_prompt = \"You are a legal expert who specializes in extracting accurate and concise holdings from case documents.\"\n",
    "\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "print(template)\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\", \"hint\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1252d5-024f-44ee-ab57-9b5e765dff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_df = pd.read_csv('case_references.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e3b34f-4d06-4afb-b833-258b600cbc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a holding based on a summary by running the langchain pipeline\n",
    "def generate_holding(filename):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    summary = read_file(file_path)\n",
    "    current_case_summary = str(summary)\n",
    "\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    case_number = int(base_name.split(\"_\")[1])\n",
    "\n",
    "    holding_hint = reference_df.iloc[case_number - 1, 6]\n",
    "    print(holding_hint)\n",
    "\n",
    "    input_dict = {'text': current_case_summary,\n",
    "                  'hint': holding_hint\n",
    "                 }\n",
    "    \n",
    "    output = llm_chain.run(input_dict)\n",
    "\n",
    "    holding_file_name = f\"{base_name}_holding.txt\"\n",
    "    \n",
    "    holding_file_path = os.path.join(output_folder, holding_file_name)\n",
    "\n",
    "    try:\n",
    "        with open(holding_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(output)\n",
    "        print(f\"Summary successfully written to {holding_file_path}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Unable to write to file: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def batch_process_holdings(batch_size, processed_files_csv):\n",
    "    \n",
    "    if os.path.exists(processed_files_csv):\n",
    "        processed_files_df = pd.read_csv(processed_files_csv)\n",
    "    else:\n",
    "        processed_files_df = pd.DataFrame(columns=[\"file_name\"])\n",
    "    \n",
    "    processed_count = 0\n",
    "    filenames = sorted(os.listdir(input_folder))\n",
    "    \n",
    "    # Start from where the last entry left off\n",
    "    last_processed_index = 0\n",
    "    if not processed_files_df.empty:\n",
    "        last_filename = processed_files_df['file_name'].iloc[-1]\n",
    "        last_processed_index = filenames.index(last_filename) + 1\n",
    "\n",
    "    # Process files starting from the last processed one\n",
    "    for filename in filenames[last_processed_index:]:\n",
    "        if processed_count >= batch_size:\n",
    "            break\n",
    "        print(\"Processing: \", filename)\n",
    "        if filename in processed_files_df['file_name'].values:\n",
    "                print(f\"File {filename} has already been processed. Skipping.\")\n",
    "                continue\n",
    "        else:\n",
    "            generate_holding(filename)\n",
    "            processed_count += 1\n",
    "\n",
    "            new_row = {\"file_name\": filename}\n",
    "            processed_files_df = pd.concat([processed_files_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            processed_files_df.to_csv(processed_files_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09299f02-53c8-4930-8751-b1a41eb5c7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'ref_case_llama_regular_summaries_small'\n",
    "output_folder = 'ref_case_llama_regular_sum_llama_holdings_small'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "processed_files_df = None\n",
    "processed_files_csv = 'processed_files_llama_regular_sum_llama_for_holdings_small.csv'\n",
    "\n",
    "for i in range(len(os.listdir(input_folder))):\n",
    "    start = time.time()\n",
    "    batch_process_holdings(1,processed_files_csv)\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed:\", end - start)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af6a9f7-0f4b-4f72-ae30-03f745738ea2",
   "metadata": {},
   "source": [
    "# Bulk Holding Generation Without Hint \n",
    "## - Using Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1b2025-c56f-4aa1-9b3d-7213ed645391",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
    "instruction = \"Use the case document to extract the concise holding. {text}\"\n",
    "system_prompt = \"You are a legal expert who specializes in extracting accurate and concise holdings from case documents.\"\n",
    "\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "print(template)\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6fe027-6ac8-4ada-853a-63cb2b6d07f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_df = pd.read_csv('case_references.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b8033b-0fe7-4416-ace3-5ad15f6daf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_holding(filename):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    summary = read_file(file_path)\n",
    "    current_case_summary = str(summary)\n",
    "\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    case_number = int(base_name.split(\"_\")[1])\n",
    "\n",
    "    holding_hint = reference_df.iloc[case_number - 1, 6]\n",
    "    print(holding_hint)\n",
    "\n",
    "    input_dict = {'text': current_case_summary,\n",
    "                  'hint': holding_hint\n",
    "                 }\n",
    "    \n",
    "    output = llm_chain.run(input_dict)\n",
    "\n",
    "    holding_file_name = f\"{base_name}_holding.txt\"\n",
    "    \n",
    "    holding_file_path = os.path.join(output_folder, holding_file_name)\n",
    "\n",
    "    try:\n",
    "        with open(holding_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(output)\n",
    "        print(f\"Summary successfully written to {holding_file_path}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Unable to write to file: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def batch_process_holdings(batch_size, processed_files_csv):\n",
    "    \n",
    "    if os.path.exists(processed_files_csv):\n",
    "        processed_files_df = pd.read_csv(processed_files_csv)\n",
    "    else:\n",
    "        processed_files_df = pd.DataFrame(columns=[\"file_name\"])\n",
    "    \n",
    "    processed_count = 0\n",
    "    filenames = sorted(os.listdir(input_folder))\n",
    "    \n",
    "    # Start from where the last entry left off\n",
    "    last_processed_index = 0\n",
    "    if not processed_files_df.empty:\n",
    "        last_filename = processed_files_df['file_name'].iloc[-1]\n",
    "        last_processed_index = filenames.index(last_filename) + 1\n",
    "\n",
    "    # Process files starting from the last processed one\n",
    "    for filename in filenames[last_processed_index:]:\n",
    "        if processed_count >= batch_size:\n",
    "            break\n",
    "        print(\"Processing: \", filename)\n",
    "        if filename in processed_files_df['file_name'].values:\n",
    "                print(f\"File {filename} has already been processed. Skipping.\")\n",
    "                continue\n",
    "        else:\n",
    "            generate_holding(filename)\n",
    "            processed_count += 1\n",
    "            new_row = {\"file_name\": filename}\n",
    "            processed_files_df = pd.concat([processed_files_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            processed_files_df.to_csv(processed_files_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a154bb-aa36-4c6d-b64e-82099ab34f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'ref_case_legalBertLarge_summaries_small'\n",
    "output_folder = 'ref_case_legalBertLarge_sum_llama_holdings_without_hint_small'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "processed_files_df = None\n",
    "processed_files_csv = 'processed_files_legalBertLarge_sum_llama_for_holdings_without_hint_small.csv'\n",
    "\n",
    "for i in range(len(os.listdir(input_folder))):\n",
    "    start = time.time()\n",
    "    batch_process_holdings(1,processed_files_csv)\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed:\", end - start)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c84f64-5703-4e37-986f-02117fe58715",
   "metadata": {},
   "source": [
    "# Bulk Holding Generation with Mistral\n",
    "## Will use between 16 and 21 Gb of GPU RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0980c365-ad66-48ac-9923-271b6a010772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "torch.set_default_device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae45533-218d-4227-aa8b-6e645bd2ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = \"./Mistral-7B-OpenOrca\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_directory,\n",
    "                                             torch_dtype=\"auto\",\n",
    "                                             device_map='auto',\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a19a9ee-0f23-4fcc-ba99-81f3559bfbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 1024,\n",
    "                do_sample=True,\n",
    "                top_k=30,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id = 3200\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c103b4-fc0c-4d53-8717-d5af7863bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# START: COPIED FROM <https://colab.research.google.com/drive/1uTJvyjhH-mvi1AmuwAOL2384X8TugGf0?usp=sharing>\n",
    "text = \"\"\"<|im_start|>system\\n\n",
    "You are MistralOrca, a large language model trained by Alignment Lab AI. Write out your reasoning step-by-step to be sure you get the right answers!\\n\n",
    "<|im_end|>\\n\n",
    "<|im_start|>user\\n\n",
    "what is the meaning of life?\\n\n",
    "<|im_end|>\"\"\"\n",
    "# END: COPIED FROM <https://colab.research.google.com/drive/1uTJvyjhH-mvi1AmuwAOL2384X8TugGf0?usp=sharing>\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "def generate_holding(filename, reference_df, instruction, system_prompt, max_length, hint_bool):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    summary = read_file(file_path)\n",
    "    current_case_summary = str(summary)\n",
    "    # print(\"\\ncurrent_case_summary: \", current_case_summary)\n",
    "\n",
    "    # Check and truncate current_case_summary if needed\n",
    "    tokens = tokenizer(current_case_summary, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    while tokens.input_ids.size(1) > max_length - 500:\n",
    "        print(\"\\n\\n\\nTruncation\")\n",
    "\n",
    "        # Truncate the text from the back\n",
    "        current_case_summary = \" \".join(current_case_summary.split(' ')[:-1])\n",
    "        # tokens = tokenizer(current_case_summary, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    case_number = int(base_name.split(\"_\")[1])\n",
    "\n",
    "    holding_hint = reference_df.iloc[case_number - 1, 6]\n",
    "\n",
    "    if hint_bool is True:\n",
    "        input_dict = {'text': current_case_summary,\n",
    "                      'hint': holding_hint\n",
    "                     }\n",
    "    else:\n",
    "        # instruction = instruction.format(text=current_case_summary)\n",
    "        input_dict = {'text': current_case_summary,\n",
    "                      'hint': holding_hint\n",
    "                     }\n",
    "    output = llm_chain.run(input_dict)\n",
    "    print(\"output: \", output)\n",
    "\n",
    "    holding_file_name = f\"{base_name}_holding.txt\"\n",
    "    \n",
    "    holding_file_path = os.path.join(output_folder, holding_file_name)\n",
    "\n",
    "    try:\n",
    "        with open(holding_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(output)\n",
    "        print(f\"Summary successfully written to {holding_file_path}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Unable to write to file: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def batch_process_holdings(batch_size, processed_files_csv, reference_df, instruction, system_prompt, max_length, hint_bool=False):\n",
    "    \n",
    "    if os.path.exists(processed_files_csv):\n",
    "        processed_files_df = pd.read_csv(processed_files_csv)\n",
    "    else:\n",
    "        processed_files_df = pd.DataFrame(columns=[\"file_name\"])\n",
    "    \n",
    "    processed_count = 0\n",
    "    filenames = sorted(os.listdir(input_folder))\n",
    "    \n",
    "    # Start from where the last entry left off\n",
    "    last_processed_index = 0\n",
    "    if not processed_files_df.empty:\n",
    "        last_filename = processed_files_df['file_name'].iloc[-1]\n",
    "        last_processed_index = filenames.index(last_filename) + 1\n",
    "\n",
    "    # Process files starting from the last processed one\n",
    "    for filename in filenames[last_processed_index:]:\n",
    "        if processed_count >= batch_size:\n",
    "            break\n",
    "        print(\"Processing: \", filename)\n",
    "        if filename in processed_files_df['file_name'].values:\n",
    "                print(f\"File {filename} has already been processed. Skipping.\")\n",
    "                continue\n",
    "        else:\n",
    "            generate_holding(filename, \n",
    "                             reference_df = reference_df, \n",
    "                             instruction = instruction,\n",
    "                             system_prompt = system_prompt,\n",
    "                             max_length = max_length,\n",
    "                             hint_bool = hint_bool\n",
    "                            )\n",
    "            processed_count += 1\n",
    "            new_row = {\"file_name\": filename}\n",
    "            processed_files_df = pd.concat([processed_files_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            processed_files_df.to_csv(processed_files_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ce4923-d34f-48cd-99c7-aefbcd5cf4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_summary = \"French Lilly further testified that some time thereafter the policy written by Mrs. Foster was mailed to him, inasmuch as plaintiff also resided in Wyoming County, and that he thereupon mailed the policy to the plaintiff, not at that time being aware of the fact that the policy was written for the same man who had been refused insurance previously by the witness. As a result of the telephone conversation, the plaintiff called upon Mrs. Foster, who issued the insurance policy in question. Later, however, the home office learned of the false answer contained in the application; and on March 8, 1957, wrote a letter to the plaintiff detailing the facts in relation to the false answer contained in the application, and notifying the plaintiff that his insurance policy was rescinded. At the time the letter of December 20 was written, the home office did not know of the accident involving the cow, or of the occurrence which resulted in the destruction of the automobile, and did not know of the fact that the application made by plaintiff contained a false statement of facts. French Lilly of Oceana, in Wyoming County, an agent for the defendant company, testified that “in the neighborhood of July in 1956”, the plaintiff showed to the witness a letter disclosing that the plaintiff’s automobile insurance with another company was being cancelled and that thereupon the witness refused to write a policy of insurance on behalf of the defendant company covering plaintiff’s automobile. As a consequence thereof, the home office wrote a letter to the plaintiff, dated December 20, 1956, three days after the automobile was demolished, notifying the plaintiff that his insurance policy would be cancelled as of January 2, 1957, and advising him to obtain proper insurance in the meantime with another company. The defendant offered the letter of March 8 for introduction in evidence, the plaintiff objected, and the court refused to permit the introduction of such letter as a part of the evidence to be considered by the jury. The defendant offered the letter of March 8 for introduction in evidence, the plaintiff objected, and the court refused to permit the introduction of such letter as a part of the evidence to be considered by the jury. Thereupon, the court made the following statement to the jury: “Ladies and gentlemen of the jury, there are two questions to be decided in this case. After the completion of the testimony at the trial before a jury, the defendant made a motion for a directed verdict in its favor, which motion was overruled. In his written opinion, which was made a part of the record, the eminent trial judge stated: “I am of the opinion that the knowledge of the agent of the company at Oceana is imputed to its principal, the defendant company, and the defendant company having had knowledge of a prior cancellation through its agent and in its principal offices, had an election to either treat the policy as void or rescind it as of a future date. The company elected to rescind and prior to the date of rescinding, the loss occurred.” Prior to the time of the trial, the defendant filed its specification of defense in accordance with the provisions of Code, 56-4-21, a portion of which was as follows: “That the plaintiff fraudulently procured the said policy of insurance by the wilful, intentional, making of false and fraudulent answers and misrepresentations upon his application for said policy of insurance, with full knowledge at the time of making said false statements and answers that the defendant would not have issued said policy of insurance had the defendant known that the statements and answers as given were false. The trial court deducted this sum from $2,800.00, the value placed on the automobile by the jury’s verdict, and entered judgment for the plaintiff for the balance, amounting to $271.45. The defendant, however, in order to save the point, took testimony in relation to the letter outside the presence and hearing of the jury, and the letter was made a part of the record. 2 syl., 140 S. E. 61; Kincaid v. Equitable Life Assur. 45 C.J.S., Insurance, Section 605, page 438. SchWarzbach v. Ohio Valley Protective Union, 25 W. Va. 622, syl. “The insurer is not precluded from setting up the falsity of answers in the application where it appears that insured knew at the time they were being written or before signing the application that they were written falsely in order to defraud the company.” 45 C.J.S., Insurance, Section 732, page 741. It has been held that, under such a state of facts, “the policy is thereby forfeited.” Saltesz v. The Sovereign Camp of the Woodmen of the World, 110 W. Va. 513, syl., 159 S. E. 513. “Plaintiff cannot, at the trial or in the appellate court, rely on an estoppel not set forth in his reply.” Capehart v. Mutual Benefit Health and Accident Ass’n., 111 W. Va. 317, syl., 161 S. E. 609. Health & Accident v. Ratcliffe, 163 Va. 325, 175 S. E. 870, 874. It has been held that, under such circumstances, “the policy will ordinarily be forfeited.” Faulkiner v. Equitable Life Insurance Co., 144, W. Va. 193, syl., 107 S. E. 2d 360. It has been held that, under such a state of facts, “the policy is thereby forfeited.” Saltesz v. The Sovereign Camp of the Woodmen of the World, 110 W. Va. 513, syl., 159 S. E. 513. Although in some jurisdictions it is held that a contract of insurance procured by fraud is void, as a general rule, such a contract is voidable at the option of insurer on discovery of the fraud.” 45 C.J.S., Insurance, Section 473(2), page 152.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beb0c3a-bca7-4af6-94dc-7f7eac8b84ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Use the case document to extract the concise holding. {text}\"\n",
    "# instruction = instruction.format(text=case_summary)\n",
    "system_prompt=\"You are MistralOrca, a legal expert who specializes in extracting accurate and concise holdings from case documents. Write out your answer short and succinct!\"\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
    "\n",
    "\n",
    "chat = [\n",
    "  {\"role\": \"system\", \"content\": system_prompt},\n",
    "  {\"role\": \"user\", \"content\": instruction}\n",
    "]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(chat, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "prompt = PromptTemplate(template=input_text, input_variables=[\"text\"])\n",
    "print(\"prompt: \", prompt)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "output = llm_chain.run(case_summary)\n",
    "print(\"output: \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f87f31-27ec-4ae0-89a5-5803f7f2330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'ref_case_legalBertLarge_summaries_small'\n",
    "output_folder = 'ref_case_legalBertLarge_sum_Mistral_holdings_without_hint_small'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "processed_files_df = None\n",
    "reference_df = pd.read_csv('case_references.csv')\n",
    "processed_files_csv = 'processed_files_legalBertLarge_sum_for_Mistral_holdings_without_hint_small.csv'\n",
    "max_length = 4096\n",
    "instruction = \"Use the case document to extract the concise holding. {text}\"\n",
    "system_prompt=\"You are MistralOrca, a legal expert who specializes in extracting accurate and concise holdings from case documents. Write out your answer short and succinct!\",\n",
    "\n",
    "for i in range(len(os.listdir(input_folder))):\n",
    "# for i in range(1):\n",
    "    start = time.time()\n",
    "    batch_process_holdings(1,processed_files_csv, reference_df, instruction, system_prompt, max_length, hint_bool=False)\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed:\", end - start)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b0b266-7c02-4db1-9655-d6b742da8b37",
   "metadata": {},
   "source": [
    "# Bulk Holding Generation with Mistral Using Hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18e80e-e7b1-4232-a5ec-6a8343d2fd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'ref_case_legalBertLarge_summaries_small'\n",
    "output_folder = 'ref_case_legalBertLarge_sum_Mistral_holdings_with_hint_small'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "processed_files_df = None\n",
    "reference_df = pd.read_csv('case_references.csv')\n",
    "processed_files_csv = 'processed_files_legalBertLarge_sum_for_Mistral_holdings_with_hint_small.csv'\n",
    "max_length = 4096\n",
    "instruction = \"Use the case document to extract the concise holding. {text} Here is a hint on the holding: {hint}\"\n",
    "system_prompt=\"You are MistralOrca, a legal expert who specializes in extracting accurate and concise holdings from case documents. Write out your answer short and succinct!\",\n",
    "\n",
    "for i in range(len(os.listdir(input_folder))):\n",
    "# for i in range(1):\n",
    "    start = time.time()\n",
    "    batch_process_holdings(1,processed_files_csv, reference_df, instruction, system_prompt, max_length, hint_bool=True)\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed:\", end - start)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49beacdb-a8b2-4c01-aa05-cae6ca42df13",
   "metadata": {},
   "source": [
    "# Longformer Bulk Summarization\n",
    "## currently does not chunk text larger than context window\n",
    "## Will modify in the future if this proves to be a viable route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329a8437-1f1a-41d2-a7a6-6fee46ac63c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LEDForConditionalGeneration, LEDTokenizer, pipeline, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d92a848-230a-425a-8d68-d8de180e8a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LEDForConditionalGeneration.from_pretrained('allenai/led-base-16384')\n",
    "tokenizer = LEDTokenizer.from_pretrained(\"allenai/led-base-16384\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fc82b0-6a28-4081-9cf7-10e00a7d597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LED_model_folder = \"./LED\"\n",
    "# os.makedirs(LED_model_folder, exist_ok=True)\n",
    "# model.save_pretrained(LED_model_folder)\n",
    "# tokenizer.save_pretrained(LED_model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c75739-b742-43a9-ba7f-15c6aca02b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LED_model_folder = \"./LED\"\n",
    "# tokenizer = LEDForConditionalGeneration.from_pretrained(LED_model_folder)\n",
    "# model = LEDTokenizer.from_pretrained(LED_model_folder)\n",
    "LED_summarizer_pipeline = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=0)  # 'device=0' to use the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342ff9bf-0fd5-4960-8937-bcf7e0e98b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_a_batch_of_case_documents(batch_size, processed_files_csv):\n",
    "    \n",
    "    if os.path.exists(processed_files_csv):\n",
    "        processed_files_df = pd.read_csv(processed_files_csv)\n",
    "    else:\n",
    "        processed_files_df = pd.DataFrame(columns=[\"file_name\"])\n",
    "    \n",
    "    processed_count = 0\n",
    "    filenames = sorted(os.listdir(input_folder))\n",
    "    \n",
    "    # Start from where the last entry left off\n",
    "    last_processed_index = 0\n",
    "    if not processed_files_df.empty:\n",
    "        last_filename = processed_files_df['file_name'].iloc[-1]\n",
    "        last_processed_index = filenames.index(last_filename) + 1\n",
    "\n",
    "    # Process files starting from the last processed one\n",
    "    for filename in filenames[last_processed_index:]:\n",
    "        if processed_count >= batch_size:\n",
    "            break\n",
    "        print(\"Processing: \", filename)\n",
    "        if filename.endswith(\".json\"):\n",
    "            if filename in processed_files_df['file_name'].values:\n",
    "                    print(f\"File {filename} has already been processed. Skipping.\")\n",
    "                    continue\n",
    "            else:\n",
    "                summarize_a_case_document(filename)\n",
    "                processed_count += 1\n",
    "                new_row = {\"file_name\": filename}\n",
    "                processed_files_df = pd.concat([processed_files_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "                processed_files_df.to_csv(processed_files_csv, index=False)\n",
    "                \n",
    "def summarize_a_case_document(filename):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    opinion = load_and_extract_data(file_path)\n",
    "    opinion = str(opinion)\n",
    "    \n",
    "    # chunk_word_count = 1000  # for instance, around 1000 words per chunk\n",
    "    # overlap_word_count = 200  # for instance, overlap of 200 words\n",
    "    \n",
    "    # chunks = chunk_text_with_overlap(opinion, chunk_word_count, overlap_word_count)\n",
    "\n",
    "    summary = LED_summarizer_pipeline(opinion, min_length=30, max_length=1000)\n",
    "    summary_text = summary[0]['summary_text']\n",
    "\n",
    "    # Save the summary to a text file\n",
    "    save_summary_to_text(summary_text, output_folder, file_path, condensed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d55caf4-e87b-4fbb-9e42-ece03dcf1923",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'ref_case_small'\n",
    "output_folder = 'ref_case_LED_summaries_small'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "processed_files_df = None\n",
    "processed_files_csv = 'processed_files_for_LED_summarizing_small.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df10321-6a59-4378-a84d-fe3a32be182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(os.listdir(input_folder))):\n",
    "    start = time.time()\n",
    "    summarize_a_batch_of_case_documents(1, processed_files_csv)\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed:\", end - start)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956b84eb-7b2a-4abd-baf6-7361c11ab4e3",
   "metadata": {},
   "source": [
    "# Long T5 Bulk Summarization\n",
    "## Currently does not chunk text larger than context window\n",
    "## Will modify in the future if this proves to be a viable route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80965a0c-dae9-4f3f-86cf-e06e0f3de8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, LongT5ForConditionalGeneration, pipeline\n",
    "model = (\n",
    "    LongT5ForConditionalGeneration.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")\n",
    "    .to(\"cuda\")\n",
    "    .half()\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62e00cd-1354-4652-9ee8-1524eb1d0cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Long_T5_summarizer_pipeline = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=0)  # 'device=0' to use the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b970be-6a12-4319-9569-716f73ec633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_a_batch_of_case_documents(batch_size, processed_files_csv, pipeline):\n",
    " \n",
    "    if os.path.exists(processed_files_csv):\n",
    "        processed_files_df = pd.read_csv(processed_files_csv)\n",
    "    else:\n",
    "        processed_files_df = pd.DataFrame(columns=[\"file_name\"])\n",
    "    \n",
    "    processed_count = 0\n",
    "    filenames = sorted(os.listdir(input_folder))\n",
    "    \n",
    "    # Start from where the last entry left off\n",
    "    last_processed_index = 0\n",
    "    if not processed_files_df.empty:\n",
    "        last_filename = processed_files_df['file_name'].iloc[-1]\n",
    "        last_processed_index = filenames.index(last_filename) + 1\n",
    "\n",
    "    # Process files starting from the last processed one\n",
    "    for filename in filenames[last_processed_index:]:\n",
    "        if processed_count >= batch_size:\n",
    "            break\n",
    "        print(\"Processing: \", filename)\n",
    "        if filename.endswith(\".json\"):\n",
    "            if filename in processed_files_df['file_name'].values:\n",
    "                    print(f\"File {filename} has already been processed. Skipping.\")\n",
    "                    continue\n",
    "            else:\n",
    "                summarize_a_case_document(filename, pipeline)\n",
    "                processed_count += 1\n",
    "\n",
    "                new_row = {\"file_name\": filename}\n",
    "                processed_files_df = pd.concat([processed_files_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "                processed_files_df.to_csv(processed_files_csv, index=False)\n",
    "                \n",
    "def summarize_a_case_document(filename, pipeline):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    opinion = load_and_extract_data(file_path)\n",
    "    opinion = str(opinion)\n",
    "    \n",
    "    # chunk_word_count = 1000  # for instance, around 1000 words per chunk\n",
    "    # overlap_word_count = 200  # for instance, overlap of 200 words\n",
    "    \n",
    "    # chunks = chunk_text_with_overlap(opinion, chunk_word_count, overlap_word_count)\n",
    "\n",
    "    summary = pipeline(opinion, min_length=30, max_length=1000)\n",
    "    summary_text = summary[0]['summary_text']\n",
    "\n",
    "    save_summary_to_text(summary_text, output_folder, file_path, condensed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c3b20b-46b7-4819-9df9-110bb8e952dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'ref_case_small'\n",
    "output_folder = 'ref_case_Long_T5_summaries_small'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "processed_files_df = None\n",
    "processed_files_csv = 'processed_files_for_Long_T5_summarizing_small.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b6213e-8442-4e01-9c0b-b644b18d0bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(os.listdir(input_folder))):\n",
    "    start = time.time()\n",
    "    summarize_a_batch_of_case_documents(1, processed_files_csv, Long_T5_summarizer_pipeline)\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed:\", end - start)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f7ffb0-a8f4-4a0f-9121-0e89b7b0831e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
