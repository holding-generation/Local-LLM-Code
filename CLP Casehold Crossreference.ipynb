{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45844b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Specify the path of your CSV file\n",
    "file_path = 'train.csv'\n",
    "\n",
    "try:\n",
    "    # Attempt to read the CSV with UTF-8 encoding first\n",
    "    df = pd.read_csv(file_path, encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    # If UTF-8 didn't work, try with \"ISO-8859-1\"\n",
    "    df = pd.read_csv(file_path, encoding='iso-8859-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6a9b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.head(5).iterrows():  # Only the first 5 rows\n",
    "        print(f\"Data point {index + 1}:\")\n",
    "\n",
    "        # Iterating through each column and printing its content.\n",
    "        for col_name in df.columns:\n",
    "            print(f\"{col_name}: {row[col_name]}\")\n",
    "\n",
    "        # Print a new line to separate the data points\n",
    "        print(\"\\n\" + \"-\"*30 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31da7d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the DataFrame is empty or if the required columns are not present\n",
    "if df.empty:\n",
    "    print(\"The DataFrame is empty. No data to print.\")\n",
    "elif df.shape[1] < 12:  # Checking if there are at least 12 columns, as we want to access up to index 11\n",
    "    print(\"The DataFrame does not have enough columns.\")\n",
    "else:\n",
    "    # Iterate through the first 5 rows\n",
    "    for index, row in df.head(5).iterrows():\n",
    "        print(f\"Data point {index + 1}:\")\n",
    "\n",
    "        col_0_data = row.iloc[1]  # Data from the first column (typically index starts from 0)\n",
    "        col_11_data = row.iloc[12]  # Data from the twelfth column\n",
    "\n",
    "        # Print the data from columns 0 and 11\n",
    "        print(f\"Column 0: {col_0_data}\")\n",
    "        print(f\"Column 11: {col_11_data}\")\n",
    "\n",
    "        # Print a new line to separate the data points\n",
    "        print(\"\\n\" + \"-\"*30 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbc01ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "num_of_words = 20  # This variable can be changed to capture more or fewer words.\n",
    "\n",
    "if df.empty:\n",
    "    print(\"The DataFrame is empty. No data to print.\")\n",
    "elif df.shape[1] < 13:\n",
    "    print(\"The DataFrame does not have enough columns.\")\n",
    "else:\n",
    "    # Iterate through the first 5 rows\n",
    "    for index, row in df.head(5).iterrows():\n",
    "        print(f\"Data point {index + 1}:\")\n",
    "\n",
    "        text_content = row.iloc[1]  # Column containing the text\n",
    "\n",
    "        # Extracting the first 'num_of_words' words from the text content\n",
    "        first_words = ' '.join(text_content.split()[:num_of_words])\n",
    "\n",
    "        # Now, for the words preceding '(<HOLDING>)', using an f-string for efficiency.\n",
    "        regex_pattern = fr'((?:\\S+\\s+){{0,{num_of_words}}})\\(<HOLDING>\\)'\n",
    "        match = re.search(regex_pattern, text_content)\n",
    "\n",
    "        print(f\"First {num_of_words} words:\")\n",
    "        print(first_words)  # Printing the first set of words\n",
    "\n",
    "        if match:\n",
    "            preceding_words = match.group(1)  # This is the string of text before your target\n",
    "            preceding_words_list = preceding_words.split()\n",
    "\n",
    "            # If there are more than 'num_of_words', we only take the last 'num_of_words' words.\n",
    "            if len(preceding_words_list) > num_of_words:\n",
    "                preceding_words_list = preceding_words_list[-num_of_words:]\n",
    "\n",
    "            print(f\"\\n{num_of_words} words before (<HOLDING>):\")\n",
    "            print(' '.join(preceding_words_list))  # Printing the words before '<HOLDING>'\n",
    "        else:\n",
    "            print(\"\\nNo '<HOLDING>' tag found in this text content.\")\n",
    "\n",
    "        print(\"\\n\" + \"-\"*30 + \"\\n\")  # Separator for clarity between data points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbb6972",
   "metadata": {},
   "source": [
    "# Code to get reference case documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e8eedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import math\n",
    "import gc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee895c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_case_citations(text, num_of_words=18):\n",
    "\n",
    "    # First, we attempt to extract 'num_of_words' before \"(<HOLDING>)\"\n",
    "    match = re.search(fr'((?:\\S+\\s+){{0,{num_of_words}}})\\(<HOLDING>\\)', text)\n",
    "    extracted_text = text  # default to full text if no specific match\n",
    "\n",
    "    if match:\n",
    "        # If a match is found, extract the specific portion of the text\n",
    "        extracted_text = ' '.join(match.group(1).split()[-num_of_words:])\n",
    "\n",
    "    # This pattern might be more complex depending on the citation formats you're dealing with\n",
    "    citation_pattern = r'(\\d+)\\s([A-Za-z0-9.]+)\\s(\\d+)'\n",
    "    matches = re.findall(citation_pattern, extracted_text)\n",
    "\n",
    "    # Build list of case citations from matches\n",
    "    case_citations = [' '.join(match) for match in matches]\n",
    "\n",
    "    if not case_citations:\n",
    "        print(\"No case citations found in the provided text.\")\n",
    "\n",
    "    return case_citations, extracted_text  \n",
    "\n",
    "def search_cases(citations):\n",
    "    \"\"\"\n",
    "    Search for cases using the first citation from a list of case citations.\n",
    "    \"\"\"\n",
    "    if not citations:\n",
    "        print(\"No citations provided.\")\n",
    "        return None\n",
    "\n",
    "    # Use the first citation in the list\n",
    "    first_citation = citations[0]\n",
    "\n",
    "    # Define the endpoint\n",
    "    url = 'https://api.case.law/v1/cases/'\n",
    "    headers = {'Authorization': f'Token {CASELAW_TOKEN}'}\n",
    "    params = {\n",
    "        'cite': first_citation,\n",
    "        'full_case': 'true'  # Requesting the full case details\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        case_data = response.json()\n",
    "        cases_results = case_data.get('results', [])\n",
    "\n",
    "        if cases_results:\n",
    "            # Assuming we only need the first case even if multiple cases match the citation\n",
    "            return cases_results[0]\n",
    "        else:\n",
    "            print(f\"No cases found for citation {first_citation}.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f'Failed to retrieve cases for citation {first_citation}: {response.status_code}')\n",
    "        return None\n",
    "\n",
    "def save_case(case, row_number, subdir='ref_case_jsons'):\n",
    "    case_id = case['id']\n",
    "    state = case['jurisdiction']['name_long']\n",
    "\n",
    "    # Save case to a JSON file\n",
    "    if not os.path.exists(subdir):\n",
    "        os.makedirs(subdir)\n",
    "    \n",
    "    with open(os.path.join(subdir, f'case_{row_number + 1}.json'), 'w') as json_file:\n",
    "        json.dump(case, json_file, indent=4)\n",
    "\n",
    "    return case_id, state\n",
    "\n",
    "def process_dataframe(df, start_row=0, num_rows_to_process=5, status_file='case_references.csv', subdir='ref_case_jsons'):\n",
    "    # Initialization\n",
    "    if os.path.exists(status_file):\n",
    "        queries_df = pd.read_csv(status_file)\n",
    "        # Ensuring we are not re-processing already processed rows\n",
    "        start_row = max(queries_df['row_number'].max() + 1, start_row)\n",
    "    else:\n",
    "        # Add new columns for the correct choice index and the corresponding answer value\n",
    "        queries_df = pd.DataFrame(columns=['row_number', \n",
    "                                           'query_status', \n",
    "                                           'case_id', \n",
    "                                           'state', 'error', \n",
    "                                           'correct_choice_index', \n",
    "                                           'correct_answer_value', \n",
    "                                           'extracted_text'])\n",
    "\n",
    "    # Main processing loop, which only processes a set number of rows\n",
    "    for row_number in range(start_row, start_row + num_rows_to_process):\n",
    "        if row_number >= len(df):\n",
    "            print(\"Reached the end of the dataframe.\")\n",
    "            break\n",
    "\n",
    "        correct_choice_index = df.iloc[row_number].iloc[12]\n",
    "\n",
    "        # Check if the value in `.iloc[12]` is valid (i.e., within the range 0-4) and not empty\n",
    "        if pd.isna(correct_choice_index) or correct_choice_index not in range(5):\n",
    "            print(f\"Skipping row {row_number} due to invalid or missing correct choice index.\")\n",
    "            continue  # Skip this row and continue with the next one\n",
    "\n",
    "        # Adjusting the column reference since .iloc[12] starts at 0 and the choices start at .iloc[2]\n",
    "        correct_answer_value = df.iloc[row_number].iloc[int(correct_choice_index) + 2]\n",
    "\n",
    "        text_content = df.iloc[row_number].iloc[1]\n",
    "        citations, extracted_text = extract_case_citations(text_content) \n",
    "        case = search_cases(citations)  # Then, search for the case based on the citation\n",
    "\n",
    "        query_status = None\n",
    "        case_id = None\n",
    "        state = None\n",
    "        error = False\n",
    "\n",
    "        if case:\n",
    "            casebody_status = case.get('casebody', {}).get('status')\n",
    "            if casebody_status == 'ok':\n",
    "                try:\n",
    "                    case_id, state = save_case(case, row_number, subdir=subdir)\n",
    "                    query_status = 1  # success\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving case for row {row_number}: {e}\")\n",
    "                    error = True\n",
    "                    query_status = 0  # failure\n",
    "            else:\n",
    "                print(f\"Casebody status is not 'ok' for row {row_number}. Exiting loop.\")\n",
    "                break  # Exit the for loop if casebody status is not \"ok\"\n",
    "        else:\n",
    "            query_status = 0  # failure or no results\n",
    "\n",
    "        # Update status dataframe with additional information about the correct choice\n",
    "        new_row = {\n",
    "            'row_number': row_number, \n",
    "            'query_status': query_status, \n",
    "            'case_id': case_id, \n",
    "            'state': state, \n",
    "            'error': error,\n",
    "            'correct_choice_index': correct_choice_index,\n",
    "            'correct_answer_value': correct_answer_value,\n",
    "            'extracted_text': extracted_text\n",
    "        }\n",
    "        queries_df = pd.concat([queries_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        print(str(row_number)+\":\",str(new_row['case_id']))\n",
    "        \n",
    "        # Save after each row to preserve data in case of interruption\n",
    "        # Attempting exponential backoff when saveing to csv\n",
    "        max_attempts = 5  # Maximum number of retry attempts\n",
    "        backoff_factor = 2  # Factor by which the wait time increases each attempt\n",
    "\n",
    "        try:\n",
    "            queries_df.to_csv(status_file, index=False)\n",
    "        except Exception as e:\n",
    "            print(\"Initial attempt failed, entering backoff loop.\")\n",
    "            for attempt in range(1, max_attempts):\n",
    "                try:\n",
    "                    time.sleep(backoff_factor ** attempt)  # Backoff before retrying\n",
    "                    queries_df.to_csv(status_file, index=False)\n",
    "                    print(f\"File saved successfully on attempt {attempt + 1}.\")\n",
    "                    break  # Exit the loop if the file is saved successfully\n",
    "                except Exception as e:\n",
    "                    print(f\"Attempt {attempt + 1} failed, retrying...\")\n",
    "            else:\n",
    "                print(\"Failed to save the file after several attempts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1084ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_case_citations(df.iloc[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd4a3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[14,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a02d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bd049d-4b53-4565-9459-d490e59776e9",
   "metadata": {},
   "source": [
    "## -getting the cases for train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeceb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "CASELAW_TOKEN = os.getenv('CASELAW_TOKEN')\n",
    "process_dataframe(df, num_rows_to_process=1,subdir='ref_case_jsons')\n",
    "\n",
    "# for i in range(25):\n",
    "#     process_dataframe(df, num_rows_to_process=500,subdir='ref_case_jsons')\n",
    "#     gc.collect()\n",
    "#     time.sleep(5)\n",
    "\n",
    "file_path = 'train.csv'\n",
    "try:\n",
    "    df = pd.read_csv(file_path, encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(file_path, encoding='iso-8859-1')\n",
    "    \n",
    "for i in range(12):\n",
    "    process_dataframe(df, num_rows_to_process=500,status_file='case_references_train.csv',subdir='ref_case_jsons_train')\n",
    "    gc.collect()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab140b",
   "metadata": {},
   "source": [
    "### - getting the cases from val.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991d7602-c36f-48e2-b873-72a1c029de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'val.csv'\n",
    "try:\n",
    "    df = pd.read_csv(file_path, encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(file_path, encoding='iso-8859-1')\n",
    "    \n",
    "for i in range(12):\n",
    "    process_dataframe(df, num_rows_to_process=500,status_file='case_references_val.csv',subdir='ref_case_jsons_val')\n",
    "    gc.collect()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89dc341",
   "metadata": {},
   "source": [
    "### - getting the cases from test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb41598c-3f52-4c36-8f4b-ab0d615a0248",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'test.csv'\n",
    "try:\n",
    "    df = pd.read_csv(file_path, encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(file_path, encoding='iso-8859-1')\n",
    "    \n",
    "for i in range(12):\n",
    "    process_dataframe(df, num_rows_to_process=500,status_file='case_references_test.csv',subdir='ref_case_jsons_test')\n",
    "    gc.collect()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3b218b",
   "metadata": {},
   "source": [
    "# Below is a version to keep on downloading even if the casebody status is not \"ok\"\n",
    "## - it'll just mark a 3 in the query_status column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33233d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe_without_regard(df, start_row=0, num_rows_to_process=5, status_file='case_references.csv'):\n",
    "    # Initialization\n",
    "    if os.path.exists(status_file):\n",
    "        queries_df = pd.read_csv(status_file)\n",
    "        # Ensuring we are not re-processing already processed rows\n",
    "        start_row = max(queries_df['row_number'].max() + 1, start_row)\n",
    "    else:\n",
    "        # Add new columns for the correct choice index and the corresponding answer value\n",
    "        queries_df = pd.DataFrame(columns=['row_number', \n",
    "                                           'query_status', \n",
    "                                           'case_id', \n",
    "                                           'state', 'error', \n",
    "                                           'correct_choice_index', \n",
    "                                           'correct_answer_value', \n",
    "                                           'extracted_text'])\n",
    "\n",
    "    # Main processing loop, which only processes a set number of rows\n",
    "    for row_number in range(start_row, start_row + num_rows_to_process):\n",
    "        if row_number >= len(df):\n",
    "            print(\"Reached the end of the dataframe.\")\n",
    "            break\n",
    "\n",
    "        correct_choice_index = df.iloc[row_number].iloc[12]\n",
    "\n",
    "        # Check if the value in `.iloc[12]` is valid (i.e., within the range 0-4) and not empty\n",
    "        if pd.isna(correct_choice_index) or correct_choice_index not in range(5):\n",
    "            print(f\"Skipping row {row_number} due to invalid or missing correct choice index.\")\n",
    "            continue  # Skip this row and continue with the next one\n",
    "\n",
    "        # Adjusting the column reference since .iloc[12] starts at 0 and the choices start at .iloc[2]\n",
    "        correct_answer_value = df.iloc[row_number].iloc[int(correct_choice_index) + 2]\n",
    "\n",
    "        text_content = df.iloc[row_number].iloc[1]\n",
    "        citations, extracted_text = extract_case_citations(text_content) \n",
    "        case = search_cases(citations)  # Then, search for the case based on the citation\n",
    "\n",
    "        query_status = None\n",
    "        case_id = None\n",
    "        state = None\n",
    "        error = False\n",
    "\n",
    "        if case:\n",
    "            casebody_status = case.get('casebody', {}).get('status')\n",
    "            if casebody_status == 'ok':\n",
    "                try:\n",
    "                    case_id, state = save_case(case, row_number)\n",
    "                    query_status = 1  # success\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving case for row {row_number}: {e}\")\n",
    "                    error = True\n",
    "                    query_status = 0  # failure\n",
    "            elif casebody_status == 'error_limit_exceeded':\n",
    "                print(f\"Data limit exceeded: null casebody for row: {row_number}\")\n",
    "                case_id, state = save_case(case, row_number)\n",
    "                query_status = 3\n",
    "            else:\n",
    "                print(f\"Casebody status is not 'ok' for row {row_number}. Exiting loop.\")\n",
    "                break  # Exit the for loop if casebody status is not \"ok\"\n",
    "        else:\n",
    "            query_status = 0  # failure or no results\n",
    "\n",
    "        # Update status dataframe with additional information about the correct choice\n",
    "        new_row = {\n",
    "            'row_number': row_number, \n",
    "            'query_status': query_status, \n",
    "            'case_id': case_id, \n",
    "            'state': state, \n",
    "            'error': error,\n",
    "            'correct_choice_index': correct_choice_index,\n",
    "            'correct_answer_value': correct_answer_value,\n",
    "            'extracted_text': extracted_text\n",
    "        }\n",
    "        queries_df = pd.concat([queries_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        print(str(row_number)+\":\",str(new_row['case_id']))\n",
    "        # Save after each row to preserve data in case of interruption\n",
    "        queries_df.to_csv(status_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d069a6-aa5d-4fc8-ba69-f90f6a9fe96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "CASELAW_TOKEN = os.getenv('CASELAW_TOKEN2')\n",
    "\n",
    "for i in range(25):\n",
    "    process_dataframe_without_regard(df, num_rows_to_process=500)\n",
    "    gc.collect()\n",
    "    time.sleep(.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731ad083",
   "metadata": {},
   "source": [
    "# Getting the unique jurisdictions from the saved list\n",
    "## this was for gathering a dataset for human annotations\n",
    "## and ensuring we had a sample from every state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6887e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "try:\n",
    "    data = pd.read_csv('case_references.csv')\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the CSV file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Check if 'state' column exists in the dataframe\n",
    "if 'state' not in data.columns:\n",
    "    print(\"'state' column not found in the CSV file\")\n",
    "    exit()\n",
    "\n",
    "# Create a new dataframe for case_key with the appropriate columns\n",
    "case_key = pd.DataFrame(columns=['case_number', 'case_id', 'jurisdiction'])\n",
    "\n",
    "# Create a set to keep track of unique states\n",
    "unique_states = set()\n",
    "\n",
    "# Iterate over every row in the original dataframe\n",
    "for index, row in data.iterrows():\n",
    "    state = row['state']\n",
    "    \n",
    "    # If we find a new state, we proceed to add the information to case_key\n",
    "    if state not in unique_states:\n",
    "        unique_states.add(state)\n",
    "        new_row = pd.DataFrame([{\n",
    "            'case_number': index + 1,  # row_number + 1\n",
    "            'case_id': row['case_id'],  # Assuming 'case_id' is the column name in your csv\n",
    "            'jurisdiction': state\n",
    "        }])\n",
    "        \n",
    "        # Concatenating the new row with the existing DataFrame\n",
    "        case_key = pd.concat([case_key, new_row], ignore_index=True)\n",
    "\n",
    "# Save the case_key dataframe\n",
    "try:\n",
    "    case_key.to_csv('case_key.csv', index=False)\n",
    "    print(\"The case_key file was saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving the case_key CSV: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d635dcb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
