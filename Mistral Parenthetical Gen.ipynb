{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd538578-088c-4c34-8b64-21f11a232b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "import json\n",
    "import textwrap\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain import PromptTemplate,  LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc\n",
    "torch.set_default_device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ddfe83-5ecb-49ce-956c-02820dfae48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = \"./Mistral-7B-OpenOrca\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_directory,\n",
    "                                             torch_dtype=\"auto\",\n",
    "                                             device_map='auto',\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e74da86-6862-44d1-b03f-203e3b7b5f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 1024,\n",
    "                do_sample=True,\n",
    "                top_k=30,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id = 3200\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec69bcb-57a5-4ccd-9f37-53063f232b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_json('cleaned_test_qlora.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab34824-3137-42eb-a2a5-82a106ba0f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Use the case document to extract the concise holding and phrase it as a parenthetical, which should look something like this: holding that the balance between costs and benefits comes out against applying the exclusionary rule in civil deportation hearings. {text}\"\n",
    "system_prompt = \"You are a legal expert who specializes in extracting accurate and concise parenthetical holdings from case documents. Give only the holdings, no other breakdowns or extra text.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca3d2af-21d2-41bb-97db-71b27c827aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = [\n",
    "  {\"role\": \"system\", \"content\": system_prompt},\n",
    "  {\"role\": \"user\", \"content\": instruction}\n",
    "]\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(chat, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "prompt = PromptTemplate(template=input_text, input_variables=[\"text\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd03b7a-9f77-4936-962f-ea5f5a336b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_test_output = llm_chain.run(test_df.iloc[0][\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f149b4b-d9a8-43ee-b917-ec2946c730f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e9be02-bfc6-4abf-9f56-6646b46ec8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_mistral = pd.DataFrame(columns=[\"Input\", \"Prediction\", \"Reference\"])\n",
    "num_nulls = 0\n",
    "\n",
    "for i in range(len(test_df)):\n",
    "    print(f\"Predicting on input number: {i}\")\n",
    "    input_txt = test_df.iloc[i][\"input\"]\n",
    "    # output_txt = llm_chain.run(input_txt)\n",
    "    \n",
    "    try:\n",
    "        output_txt = llm_chain.run(input_txt)\n",
    "    except RuntimeError:\n",
    "        print(\"Generation failed, inserting NULL value\")\n",
    "        output_txt = \"NULL\"\n",
    "        num_nulls += 1\n",
    "    \n",
    "    reference_txt = test_df.iloc[i][\"output\"]\n",
    "    \n",
    "    temp_df = pd.DataFrame({'Input': [input_txt], 'Prediction': [output_txt], 'Reference': [reference_txt]})\n",
    "    \n",
    "    results_df_mistral = pd.concat([results_df_mistral, temp_df], ignore_index=True)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "print(\"Inference has finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848c2c8a-b51b-4183-a8bc-420c9655b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The number of nulls inserted is {num_nulls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456f50e3-eb63-4f4d-9d33-5cf5263ad42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_mistral.to_csv('mistral_parenthetical_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1128ae69-85d8-4a34-8c04-d68dc8d6d795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
