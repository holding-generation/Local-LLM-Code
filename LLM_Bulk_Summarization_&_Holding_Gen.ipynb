{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55ffb1dd-f870-4379-8eef-88ddc792541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "import json\n",
    "import textwrap\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain import PromptTemplate,  LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0b0979e-d803-43ae-b372-df5dd72bc355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HF_AUTH_TOKEN\")\n",
    "login(token=HUGGINGFACEHUB_API_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7095cb30-54d8-4b74-9f54-02a0b2d9df16",
   "metadata": {},
   "source": [
    "# Skip the next two cells if you don't want to use Llama 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b59adc15-db39-4fbd-a8d2-486cb44056d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = \"./Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_directory,\n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             # load_in_8bit=True,\n",
    "                                             load_in_4bit=True\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a579b9e-84e1-424a-8774-251060a33fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 1024,\n",
    "                do_sample=True,\n",
    "                top_k=30,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1531e1d-6199-4eac-b758-56047f7ec343",
   "metadata": {},
   "source": [
    "# The cell below has important functions\n",
    "## It's best to run it, regardless of which task and model you're using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbe50231-e60c-4233-b32e-15e729c042c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
    "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "    return prompt_template\n",
    "\n",
    "def cut_off_text(text, prompt):\n",
    "    cutoff_phrase = prompt\n",
    "    index = text.find(cutoff_phrase)\n",
    "    if index != -1:\n",
    "        return text[:index]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_substring(string, substring):\n",
    "    return string.replace(substring, \"\")\n",
    "\n",
    "\n",
    "\n",
    "def generate(text):\n",
    "    prompt = get_prompt(text)\n",
    "    with torch.autocast('cuda', dtype=torch.bfloat16):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "        outputs = model.generate(**inputs,\n",
    "                                 max_new_tokens=1024,\n",
    "                                 eos_token_id=tokenizer.eos_token_id,\n",
    "                                 pad_token_id=tokenizer.eos_token_id,\n",
    "                                 )\n",
    "        final_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        final_outputs = cut_off_text(final_outputs, '</s>')\n",
    "        final_outputs = remove_substring(final_outputs, prompt)\n",
    "\n",
    "    return final_outputs#, outputs\n",
    "\n",
    "def parse_text(text):\n",
    "        wrapped_text = textwrap.fill(text, width=100)\n",
    "        print(wrapped_text +'\\n\\n')\n",
    "        # return assistant_text\n",
    "\n",
    "def count_words(input_string):\n",
    "    words = input_string.split(\" \")\n",
    "    return len(words)\n",
    "\n",
    "def summarize_chunks(chunks, model, tokenizer):\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        output = llm_chain.run(chunk)\n",
    "        # print(count_words(output))\n",
    "        # parse_text(output)\n",
    "        summaries.append(output)\n",
    "    return summaries\n",
    "\n",
    "def create_final_summary(summaries):\n",
    "    # Option 1: Just join the summaries\n",
    "    final_summary = ' '.join(summaries)\n",
    "\n",
    "    # Option 2: Apply another round of summarization (can be useful for coherence)\n",
    "    # final_summary = generate(final_summary)  # This is recursive and might degrade quality\n",
    "\n",
    "    return final_summary\n",
    "\n",
    "def chunk_text_with_overlap(text, chunk_word_count, overlap_word_count):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    index = 0\n",
    "\n",
    "    while index < len(words):\n",
    "        # Calculate the end index for the current chunk\n",
    "        current_chunk_end = index + chunk_word_count\n",
    "        \n",
    "        # We don't want to overshoot the list of words for the current chunk\n",
    "        current_chunk_end = min(current_chunk_end, len(words))\n",
    "\n",
    "        # Create the chunk\n",
    "        chunk = \" \".join(words[index:current_chunk_end])\n",
    "        chunks.append(chunk)\n",
    "\n",
    "        # Calculate the start index for the next chunk (considering overlap)\n",
    "        index += chunk_word_count - overlap_word_count\n",
    "\n",
    "        # If the calculated index doesn't advance (due to large overlap), we force it to advance to avoid an infinite loop\n",
    "        if index >= current_chunk_end:\n",
    "            index = current_chunk_end\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Function to load data from the JSON file and extract the desired information.\n",
    "def load_and_extract_data(file_path):\n",
    "    # Reading the file.\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)  # Parsing the JSON data.\n",
    "\n",
    "    for o in data[\"casebody\"][\"data\"][\"opinions\"]:\n",
    "        if o[\"type\"] == \"majority\":\n",
    "            return o[\"text\"]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def save_summary_to_text(summary, output_folder, file_path, condensed=False):\n",
    "    \"\"\"\n",
    "    Save the content of 'summary' to a text file derived from the name of the input file.\n",
    "    \"\"\"\n",
    "    # Extract the base file name without extension\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "    # Construct the new file name for the summary\n",
    "    if condensed:\n",
    "        summary_file_name = f\"{base_name}_condensed_summary.txt\"\n",
    "    else:\n",
    "        summary_file_name = f\"{base_name}_summary.txt\"\n",
    "    \n",
    "    # Construct the full path for the summary file\n",
    "    summary_file_path = os.path.join(output_folder, summary_file_name)\n",
    "\n",
    "    try:\n",
    "        # Using 'with' for proper file closure\n",
    "        with open(summary_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(summary)\n",
    "        print(f\"Summary successfully written to {summary_file_name}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Unable to write to file: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def read_file(file_path):\n",
    "    \"\"\"\n",
    "    Read the content of a text file.\n",
    "\n",
    "    :param file_path: str, path to the file to read.\n",
    "    :return: str, content of the file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        return content\n",
    "    except IOError as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab072408-6e04-4556-9026-30e9dca50209",
   "metadata": {},
   "source": [
    "# Bulk Opinion Summarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cd11c45-118d-4857-922a-5a7b4fd8c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
    "instruction = \"Summarize the following case document. Remember to include the relevant facts and rules of the case. {text}\"\n",
    "system_prompt = \"You are a legal expert who specializes in summarizing case documents while including the legal facts.\"\n",
    "\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "812c93ac-f8c2-46ac-8e6f-0f823bcbf93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'ref_case_small'\n",
    "output_folder = 'ref_case_summaries_small'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "processed_files_df = None\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
    "instruction = \"Summarize the following case document. Remember to include the relevant facts and rules of the case. {text}\"\n",
    "system_prompt = \"You are a legal expert who specializes in summarizing case documents while including the legal facts.\"\n",
    "\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "def summarize_a_batch_of_case_documents(batch_size):\n",
    "    # Define the filename for the processed files DataFrame\n",
    "    processed_files_csv = 'processed_files_for_summarizing_small.csv'\n",
    "    \n",
    "    # Check if the CSV file exists and load it, otherwise create an empty DataFrame\n",
    "    if os.path.exists(processed_files_csv):\n",
    "        processed_files_df = pd.read_csv(processed_files_csv)\n",
    "    else:\n",
    "        processed_files_df = pd.DataFrame(columns=[\"file_name\"])\n",
    "    \n",
    "    # Counter for the number of processed files in the batch\n",
    "    processed_count = 0\n",
    "\n",
    "    # Retrieve all filenames, sorted to ensure consistency across runs\n",
    "    filenames = sorted(os.listdir(input_folder))\n",
    "    \n",
    "    # Start from where the last entry left off\n",
    "    last_processed_index = 0\n",
    "    if not processed_files_df.empty:\n",
    "        last_filename = processed_files_df['file_name'].iloc[-1]\n",
    "        last_processed_index = filenames.index(last_filename) + 1\n",
    "\n",
    "    # Process files starting from the last processed one\n",
    "    for filename in filenames[last_processed_index:]:\n",
    "        if processed_count >= batch_size:\n",
    "            break\n",
    "        print(\"Processing: \", filename)\n",
    "        if filename.endswith(\".json\"):\n",
    "            if filename in processed_files_df['file_name'].values:\n",
    "                    print(f\"File {filename} has already been processed. Skipping.\")\n",
    "                    continue\n",
    "            else:\n",
    "                summarize_a_case_document(filename)\n",
    "                processed_count += 1  # Increment the processed files counter\n",
    "\n",
    "                # Add the file to the processed DataFrame\n",
    "                new_row = {\"file_name\": filename}\n",
    "                processed_files_df = pd.concat([processed_files_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "                processed_files_df.to_csv(processed_files_csv, index=False)\n",
    "                \n",
    "def summarize_a_case_document(filename):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    opinion = load_and_extract_data(file_path)\n",
    "    opinion = str(opinion)\n",
    "    \n",
    "    chunk_word_count = 1000  # for instance, around 1000 words per chunk\n",
    "    overlap_word_count = 200  # for instance, overlap of 200 words\n",
    "    \n",
    "    chunks = chunk_text_with_overlap(opinion, chunk_word_count, overlap_word_count)\n",
    "\n",
    "    chunk_summaries = summarize_chunks(chunks, model, tokenizer)\n",
    "    final_summary = create_final_summary(chunk_summaries)\n",
    "    \n",
    "    # Save the summary to a text file\n",
    "    save_summary_to_text(final_summary, output_folder, file_path, condensed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2855513d-8248-48eb-8c3b-30d6943684a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  case_103.json\n",
      "Summary successfully written to case_103_summary.txt\n",
      "Time elapsed: 1371.5429515838623\n",
      "Processing:  case_104.json\n",
      "Summary successfully written to case_104_summary.txt\n",
      "Time elapsed: 486.6129982471466\n",
      "Processing:  case_116.json\n",
      "Summary successfully written to case_116_summary.txt\n",
      "Time elapsed: 621.269606590271\n",
      "Processing:  case_135.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_135_summary.txt\n",
      "Time elapsed: 393.5242073535919\n",
      "Processing:  case_146.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_146_summary.txt\n",
      "Time elapsed: 743.0797629356384\n",
      "Processing:  case_175.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_175_summary.txt\n",
      "Time elapsed: 914.5540976524353\n",
      "Processing:  case_225.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_225_summary.txt\n",
      "Time elapsed: 552.0695128440857\n",
      "Processing:  case_231.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_231_summary.txt\n",
      "Time elapsed: 751.2099440097809\n",
      "Processing:  case_235.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_235_summary.txt\n",
      "Time elapsed: 732.7441337108612\n",
      "Processing:  case_242.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_242_summary.txt\n",
      "Time elapsed: 1061.977353811264\n",
      "Processing:  case_249.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_249_summary.txt\n",
      "Time elapsed: 1015.3821969032288\n",
      "Processing:  case_278.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_278_summary.txt\n",
      "Time elapsed: 754.6447155475616\n",
      "Processing:  case_284.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_284_summary.txt\n",
      "Time elapsed: 518.210547208786\n",
      "Processing:  case_289.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_289_summary.txt\n",
      "Time elapsed: 867.6442766189575\n",
      "Processing:  case_316.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_316_summary.txt\n",
      "Time elapsed: 812.6879885196686\n",
      "Processing:  case_79.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_79_summary.txt\n",
      "Time elapsed: 1034.909318447113\n"
     ]
    }
   ],
   "source": [
    "for i in range(16):\n",
    "    start = time.time()\n",
    "    summarize_a_batch_of_case_documents(1)\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed:\", end - start)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e07d1a0e-2af8-4040-b205-3519c18457e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Wed Nov  1 21:52:09 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.07             Driver Version: 537.34       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090 Ti     On  | 00000000:06:00.0  On |                  Off |\n",
      "|  0%   51C    P5              46W / 450W |  10346MiB / 24564MiB |     23%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      9450      C   /python3.10                               N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba358405-55aa-47e6-bc03-5edd90af020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10500f2c-c025-48ea-bf9c-b25e59e6d2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Wed Nov  1 21:52:09 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.07             Driver Version: 537.34       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090 Ti     On  | 00000000:06:00.0  On |                  Off |\n",
      "|  0%   51C    P5              45W / 450W |  10346MiB / 24564MiB |      5%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      9450      C   /python3.10                               N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3715298b-39c6-46ca-b275-49033e8933c7",
   "metadata": {},
   "source": [
    "## Bulk Summarization without special prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a658beb-719b-4e61-a699-e6c4b9d4608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_a_batch_of_case_documents(batch_size, processed_files_csv):\n",
    "    # Check if the CSV file exists and load it, otherwise create an empty DataFrame\n",
    "    if os.path.exists(processed_files_csv):\n",
    "        processed_files_df = pd.read_csv(processed_files_csv)\n",
    "    else:\n",
    "        processed_files_df = pd.DataFrame(columns=[\"file_name\"])\n",
    "    \n",
    "    # Counter for the number of processed files in the batch\n",
    "    processed_count = 0\n",
    "\n",
    "    # Retrieve all filenames, sorted to ensure consistency across runs\n",
    "    filenames = sorted(os.listdir(input_folder))\n",
    "    \n",
    "    # Start from where the last entry left off\n",
    "    last_processed_index = 0\n",
    "    if not processed_files_df.empty:\n",
    "        last_filename = processed_files_df['file_name'].iloc[-1]\n",
    "        last_processed_index = filenames.index(last_filename) + 1\n",
    "\n",
    "    # Process files starting from the last processed one\n",
    "    for filename in filenames[last_processed_index:]:\n",
    "        if processed_count >= batch_size:\n",
    "            break\n",
    "        print(\"Processing: \", filename)\n",
    "        if filename.endswith(\".json\"):\n",
    "            if filename in processed_files_df['file_name'].values:\n",
    "                    print(f\"File {filename} has already been processed. Skipping.\")\n",
    "                    continue\n",
    "            else:\n",
    "                summarize_a_case_document(filename)\n",
    "                processed_count += 1  # Increment the processed files counter\n",
    "\n",
    "                # Add the file to the processed DataFrame\n",
    "                new_row = {\"file_name\": filename}\n",
    "                processed_files_df = pd.concat([processed_files_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "                processed_files_df.to_csv(processed_files_csv, index=False)\n",
    "                \n",
    "def summarize_a_case_document(filename):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    opinion = load_and_extract_data(file_path)\n",
    "    opinion = str(opinion)\n",
    "    \n",
    "    chunk_word_count = 1000  # for instance, around 1000 words per chunk\n",
    "    overlap_word_count = 200  # for instance, overlap of 200 words\n",
    "    \n",
    "    chunks = chunk_text_with_overlap(opinion, chunk_word_count, overlap_word_count)\n",
    "\n",
    "    chunk_summaries = summarize_chunks(chunks, model, tokenizer)\n",
    "    final_summary = create_final_summary(chunk_summaries)\n",
    "    \n",
    "    # Save the summary to a text file\n",
    "    save_summary_to_text(final_summary, output_folder, file_path, condensed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "890faeaf-4e0d-4431-b64e-df1d58e7d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
    "instruction = \"Summarize the following: {text}\"\n",
    "system_prompt = \"You are an expert summarizier.\"\n",
    "\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f932752-0e84-4345-8285-0f90e7ab6748",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'ref_case_small'\n",
    "output_folder = 'ref_case_llama_regular_summaries_small'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "processed_files_df = None\n",
    "# Define the filename for the processed files DataFrame\n",
    "processed_files_csv = 'processed_files_for_llama_regular_summarizing_small.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f500acfa-d1cb-48a6-a763-24180fb0f568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  case_103.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_103_summary.txt\n",
      "Time elapsed: 936.8628714084625\n",
      "Processing:  case_104.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_104_summary.txt\n",
      "Time elapsed: 276.73224091529846\n",
      "Processing:  case_116.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_116_summary.txt\n",
      "Time elapsed: 399.6183753013611\n",
      "Processing:  case_135.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_135_summary.txt\n",
      "Time elapsed: 266.5895538330078\n",
      "Processing:  case_146.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_146_summary.txt\n",
      "Time elapsed: 208.6289381980896\n",
      "Processing:  case_175.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_175_summary.txt\n",
      "Time elapsed: 723.4104678630829\n",
      "Processing:  case_225.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_225_summary.txt\n",
      "Time elapsed: 446.85349225997925\n",
      "Processing:  case_231.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_231_summary.txt\n",
      "Time elapsed: 496.8048174381256\n",
      "Processing:  case_235.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_235_summary.txt\n",
      "Time elapsed: 495.87283086776733\n",
      "Processing:  case_242.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_242_summary.txt\n",
      "Time elapsed: 909.6987643241882\n",
      "Processing:  case_249.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_249_summary.txt\n",
      "Time elapsed: 664.7188773155212\n",
      "Processing:  case_278.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_278_summary.txt\n",
      "Time elapsed: 508.27871775627136\n",
      "Processing:  case_284.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_284_summary.txt\n",
      "Time elapsed: 212.46659922599792\n",
      "Processing:  case_289.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_289_summary.txt\n",
      "Time elapsed: 584.2650899887085\n",
      "Processing:  case_316.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_316_summary.txt\n",
      "Time elapsed: 525.4488999843597\n",
      "Processing:  case_335.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_335_summary.txt\n",
      "Time elapsed: 220.56410121917725\n",
      "Processing:  case_352.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_352_summary.txt\n",
      "Time elapsed: 637.4101104736328\n",
      "Processing:  case_79.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_79_summary.txt\n",
      "Time elapsed: 551.267014503479\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(os.listdir(input_folder))):\n",
    "    start = time.time()\n",
    "    summarize_a_batch_of_case_documents(1, processed_files_csv)\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed:\", end - start)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace1e305-8dd1-44ae-8492-de42ce50e235",
   "metadata": {},
   "source": [
    "# Bulk Holding Generation with Llama 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7330935f-f197-49a0-bec5-3076f1476639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\n",
      "You are a legal expert who specializes in extracting accurate and concise holdings from case documents.\n",
      "<</SYS>>\n",
      "\n",
      "Use the case document to extract the concise holding. {text} Here is a hint on the holding: {hint}[/INST]\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
    "instruction = \"Use the case document to extract the concise holding. {text} Here is a hint on the holding: {hint}\"\n",
    "system_prompt = \"You are a legal expert who specializes in extracting accurate and concise holdings from case documents.\"\n",
    "\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "print(template)\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\", \"hint\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa1252d5-024f-44ee-ab57-9b5e765dff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_df = pd.read_csv('case_references.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0e3b34f-4d06-4afb-b833-258b600cbc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_holding(filename):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    summary = read_file(file_path)\n",
    "    current_case_summary = str(summary)\n",
    "\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    case_number = int(base_name.split(\"_\")[1])\n",
    "\n",
    "    holding_hint = reference_df.iloc[case_number - 1, 6]\n",
    "    print(holding_hint)\n",
    "\n",
    "    input_dict = {'text': current_case_summary,\n",
    "                  'hint': holding_hint\n",
    "                 }\n",
    "    \n",
    "    output = llm_chain.run(input_dict)\n",
    "\n",
    "    holding_file_name = f\"{base_name}_holding.txt\"\n",
    "    \n",
    "    # Construct the full path for the summary file\n",
    "    holding_file_path = os.path.join(output_folder, holding_file_name)\n",
    "\n",
    "    try:\n",
    "        # Using 'with' for proper file closure\n",
    "        with open(holding_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(output)\n",
    "        print(f\"Summary successfully written to {holding_file_path}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Unable to write to file: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def batch_process_holdings(batch_size, processed_files_csv):\n",
    "    \n",
    "    # Check if the CSV file exists and load it, otherwise create an empty DataFrame\n",
    "    if os.path.exists(processed_files_csv):\n",
    "        processed_files_df = pd.read_csv(processed_files_csv)\n",
    "    else:\n",
    "        processed_files_df = pd.DataFrame(columns=[\"file_name\"])\n",
    "    \n",
    "    # Counter for the number of processed files in the batch\n",
    "    processed_count = 0\n",
    "\n",
    "    # Retrieve all filenames, sorted to ensure consistency across runs\n",
    "    filenames = sorted(os.listdir(input_folder))\n",
    "    \n",
    "    # Start from where the last entry left off\n",
    "    last_processed_index = 0\n",
    "    if not processed_files_df.empty:\n",
    "        last_filename = processed_files_df['file_name'].iloc[-1]\n",
    "        last_processed_index = filenames.index(last_filename) + 1\n",
    "\n",
    "    # Process files starting from the last processed one\n",
    "    for filename in filenames[last_processed_index:]:\n",
    "        if processed_count >= batch_size:\n",
    "            break\n",
    "        print(\"Processing: \", filename)\n",
    "        if filename in processed_files_df['file_name'].values:\n",
    "                print(f\"File {filename} has already been processed. Skipping.\")\n",
    "                continue\n",
    "        else:\n",
    "            generate_holding(filename)\n",
    "            processed_count += 1  # Increment the processed files counter\n",
    "\n",
    "            # Add the file to the processed DataFrame\n",
    "            new_row = {\"file_name\": filename}\n",
    "            processed_files_df = pd.concat([processed_files_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            processed_files_df.to_csv(processed_files_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09299f02-53c8-4930-8751-b1a41eb5c7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  case_103_summary.txt\n",
      "holding state as parens patriae takes strong interest in care and treatment of children within its borders\n",
      "Summary successfully written to ref_case_llama_regular_sum_llama_holdings_small/case_103_summary_holding.txt\n",
      "Time elapsed: 76.03627490997314\n",
      "Processing:  case_104_summary.txt\n",
      "holding in addition that offense of obstructing a law enforcement officer did not meet therein element of burglary\n",
      "Summary successfully written to ref_case_llama_regular_sum_llama_holdings_small/case_104_summary_holding.txt\n",
      "Time elapsed: 69.09998416900635\n",
      "Processing:  case_116_summary.txt\n",
      "holding that proximate cause is legal concept with particular meaning in law and is not in category of words or phrases commonly known and understood by lay public thus it was error not to give instruction defining it\n",
      "Summary successfully written to ref_case_llama_regular_sum_llama_holdings_small/case_116_summary_holding.txt\n",
      "Time elapsed: 47.456737756729126\n",
      "Processing:  case_135_summary.txt\n",
      "recognizing that an owner of property could be personally liable to one who voluntarily pays ad valorem taxes if the parties executed a valid contract or agreement\n",
      "Summary successfully written to ref_case_llama_regular_sum_llama_holdings_small/case_135_summary_holding.txt\n",
      "Time elapsed: 63.984601974487305\n",
      "Processing:  case_146_summary.txt\n",
      "holding student loan proceeds did not lose exempt status after excess of proceeds after payment of tuition and fees was deposited in students bank account\n",
      "Summary successfully written to ref_case_llama_regular_sum_llama_holdings_small/case_146_summary_holding.txt\n",
      "Time elapsed: 18.39682412147522\n",
      "Processing:  case_175_summary.txt\n",
      "holding that appellants testimony negated the intoxication defense by demonstrating he was in control of his mental faculties\n",
      "Summary successfully written to ref_case_llama_regular_sum_llama_holdings_small/case_175_summary_holding.txt\n",
      "Time elapsed: 137.37973260879517\n",
      "Processing:  case_225_summary.txt\n",
      "holding the commission cannot rely on a va rating to find a claimant was totally disabled\n",
      "Summary successfully written to ref_case_llama_regular_sum_llama_holdings_small/case_225_summary_holding.txt\n",
      "Time elapsed: 60.66717886924744\n",
      "Processing:  case_231_summary.txt\n",
      "recognizing that a dismissal without prejudice along with a limited opportunity to amend the complaint within twenty days only became final and binding when no amended pleadings were filed within the time period allowed\n",
      "Summary successfully written to ref_case_llama_regular_sum_llama_holdings_small/case_231_summary_holding.txt\n",
      "Time elapsed: 118.04621076583862\n",
      "Processing:  case_235_summary.txt\n",
      "holding that a claim for interest on a money judgment did not accrue until the judgment was rendered which was the date on which the trial court issued its order\n",
      "Summary successfully written to ref_case_llama_regular_sum_llama_holdings_small/case_235_summary_holding.txt\n",
      "Time elapsed: 22.50246524810791\n",
      "Processing:  case_242_summary.txt\n",
      "holding that a court may not rewrite the insurance contract under the guise of judicial interpretation\n",
      "Summary successfully written to ref_case_llama_regular_sum_llama_holdings_small/case_242_summary_holding.txt\n",
      "Time elapsed: 75.8449547290802\n",
      "Processing:  case_249_summary.txt\n",
      "recognizing that intentional furnishing of false information of a material nature is breach of cooperation clause and discussing misrepresentations provided during investigation and trial of claim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to ref_case_llama_regular_sum_llama_holdings_small/case_249_summary_holding.txt\n",
      "Time elapsed: 92.91756248474121\n",
      "Processing:  case_278_summary.txt\n",
      "holding that fixing tickets by passing them to the inactive files without requiring the defendants to appear in court and over the objections of the issuing officer constituted moral turpitude\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to ref_case_llama_regular_sum_llama_holdings_small/case_278_summary_holding.txt\n",
      "Time elapsed: 61.606207609176636\n",
      "Processing:  case_284_summary.txt\n",
      "holding similarly where appellant claimed that she was entitled to recover attorney fees under nrs 18010 even though nrcp 68f and nrs 171154 foreclosed such a recovery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to ref_case_llama_regular_sum_llama_holdings_small/case_284_summary_holding.txt\n",
      "Time elapsed: 50.36093878746033\n",
      "Processing:  case_289_summary.txt\n",
      "holding that juveniles may waive constitutional rights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to ref_case_llama_regular_sum_llama_holdings_small/case_289_summary_holding.txt\n",
      "Time elapsed: 129.9863474369049\n",
      "Processing:  case_316_summary.txt\n",
      "holding that article i section 19 of the missouri constitution requires that once a witness claims the privilege against selfinerimination afforded by that provision a rebuttable presumption arises that the witness answer might tend to incriminate him\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to ref_case_llama_regular_sum_llama_holdings_small/case_316_summary_holding.txt\n",
      "Time elapsed: 59.22045588493347\n",
      "Processing:  case_335_summary.txt\n",
      "recognizing if truth about paternity can be discovered and equity does not demand otherwise presumption of legitimacy should not be used to perpetuate a falsehood\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to ref_case_llama_regular_sum_llama_holdings_small/case_335_summary_holding.txt\n",
      "Time elapsed: 109.90361285209656\n",
      "Processing:  case_352_summary.txt\n",
      "holding fjraud on the part of the insured in the procurement of the policy  is sufficient to defeat a recovery in an action on such policy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to ref_case_llama_regular_sum_llama_holdings_small/case_352_summary_holding.txt\n",
      "Time elapsed: 64.18657112121582\n",
      "Processing:  case_79_summary.txt\n",
      "holding that appeals are taken from judgments and not from opinions let alone dicta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to ref_case_llama_regular_sum_llama_holdings_small/case_79_summary_holding.txt\n",
      "Time elapsed: 54.9709587097168\n"
     ]
    }
   ],
   "source": [
    "input_folder = 'ref_case_llama_regular_summaries_small'\n",
    "output_folder = 'ref_case_llama_regular_sum_llama_holdings_small'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "processed_files_df = None\n",
    "# Define the filename for the processed files DataFrame\n",
    "processed_files_csv = 'processed_files_llama_regular_sum_llama_for_holdings_small.csv'\n",
    "\n",
    "for i in range(len(os.listdir(input_folder))):\n",
    "    start = time.time()\n",
    "    batch_process_holdings(1,processed_files_csv)\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed:\", end - start)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af6a9f7-0f4b-4f72-ae30-03f745738ea2",
   "metadata": {},
   "source": [
    "# Bulk Holding Generation Without Hint \n",
    "## - Using Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe1b2025-c56f-4aa1-9b3d-7213ed645391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\n",
      "You are a legal expert who specializes in extracting accurate and concise holdings from case documents.\n",
      "<</SYS>>\n",
      "\n",
      "Use the case document to extract the concise holding. {text}[/INST]\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
    "instruction = \"Use the case document to extract the concise holding. {text}\"\n",
    "system_prompt = \"You are a legal expert who specializes in extracting accurate and concise holdings from case documents.\"\n",
    "\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "print(template)\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d6fe027-6ac8-4ada-853a-63cb2b6d07f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_df = pd.read_csv('case_references.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2b8033b-0fe7-4416-ace3-5ad15f6daf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_holding(filename):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    summary = read_file(file_path)\n",
    "    current_case_summary = str(summary)\n",
    "\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    case_number = int(base_name.split(\"_\")[1])\n",
    "\n",
    "    holding_hint = reference_df.iloc[case_number - 1, 6]\n",
    "    print(holding_hint)\n",
    "\n",
    "    input_dict = {'text': current_case_summary,\n",
    "                  'hint': holding_hint\n",
    "                 }\n",
    "    \n",
    "    output = llm_chain.run(input_dict)\n",
    "\n",
    "    holding_file_name = f\"{base_name}_holding.txt\"\n",
    "    \n",
    "    # Construct the full path for the summary file\n",
    "    holding_file_path = os.path.join(output_folder, holding_file_name)\n",
    "\n",
    "    try:\n",
    "        # Using 'with' for proper file closure\n",
    "        with open(holding_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(output)\n",
    "        print(f\"Summary successfully written to {holding_file_path}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Unable to write to file: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def batch_process_holdings(batch_size, processed_files_csv):\n",
    "    \n",
    "    # Check if the CSV file exists and load it, otherwise create an empty DataFrame\n",
    "    if os.path.exists(processed_files_csv):\n",
    "        processed_files_df = pd.read_csv(processed_files_csv)\n",
    "    else:\n",
    "        processed_files_df = pd.DataFrame(columns=[\"file_name\"])\n",
    "    \n",
    "    # Counter for the number of processed files in the batch\n",
    "    processed_count = 0\n",
    "\n",
    "    # Retrieve all filenames, sorted to ensure consistency across runs\n",
    "    filenames = sorted(os.listdir(input_folder))\n",
    "    \n",
    "    # Start from where the last entry left off\n",
    "    last_processed_index = 0\n",
    "    if not processed_files_df.empty:\n",
    "        last_filename = processed_files_df['file_name'].iloc[-1]\n",
    "        last_processed_index = filenames.index(last_filename) + 1\n",
    "\n",
    "    # Process files starting from the last processed one\n",
    "    for filename in filenames[last_processed_index:]:\n",
    "        if processed_count >= batch_size:\n",
    "            break\n",
    "        print(\"Processing: \", filename)\n",
    "        if filename in processed_files_df['file_name'].values:\n",
    "                print(f\"File {filename} has already been processed. Skipping.\")\n",
    "                continue\n",
    "        else:\n",
    "            generate_holding(filename)\n",
    "            processed_count += 1  # Increment the processed files counter\n",
    "\n",
    "            # Add the file to the processed DataFrame\n",
    "            new_row = {\"file_name\": filename}\n",
    "            processed_files_df = pd.concat([processed_files_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            processed_files_df.to_csv(processed_files_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2a154bb-aa36-4c6d-b64e-82099ab34f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  case_103_summary.txt\n",
      "holding state as parens patriae takes strong interest in care and treatment of children within its borders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to ref_case_legalBertLarge_sum_llama_holdings_without_hint_small/case_103_summary_holding.txt\n",
      "Time elapsed: 61.34729218482971\n",
      "Processing:  case_104_summary.txt\n",
      "holding in addition that offense of obstructing a law enforcement officer did not meet therein element of burglary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to ref_case_legalBertLarge_sum_llama_holdings_without_hint_small/case_104_summary_holding.txt\n",
      "Time elapsed: 55.870909214019775\n",
      "Processing:  case_116_summary.txt\n",
      "holding that proximate cause is legal concept with particular meaning in law and is not in category of words or phrases commonly known and understood by lay public thus it was error not to give instruction defining it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to ref_case_legalBertLarge_sum_llama_holdings_without_hint_small/case_116_summary_holding.txt\n",
      "Time elapsed: 63.08406114578247\n",
      "Processing:  case_135_summary.txt\n",
      "recognizing that an owner of property could be personally liable to one who voluntarily pays ad valorem taxes if the parties executed a valid contract or agreement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to ref_case_legalBertLarge_sum_llama_holdings_without_hint_small/case_135_summary_holding.txt\n",
      "Time elapsed: 55.00631761550903\n",
      "Processing:  case_146_summary.txt\n",
      "holding student loan proceeds did not lose exempt status after excess of proceeds after payment of tuition and fees was deposited in students bank account\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to ref_case_legalBertLarge_sum_llama_holdings_without_hint_small/case_146_summary_holding.txt\n",
      "Time elapsed: 51.81976509094238\n",
      "Processing:  case_175_summary.txt\n",
      "holding that appellants testimony negated the intoxication defense by demonstrating he was in control of his mental faculties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to ref_case_legalBertLarge_sum_llama_holdings_without_hint_small/case_175_summary_holding.txt\n",
      "Time elapsed: 89.6313304901123\n",
      "Processing:  case_225_summary.txt\n",
      "holding the commission cannot rely on a va rating to find a claimant was totally disabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to ref_case_legalBertLarge_sum_llama_holdings_without_hint_small/case_225_summary_holding.txt\n",
      "Time elapsed: 89.94023299217224\n",
      "Processing:  case_231_summary.txt\n",
      "recognizing that a dismissal without prejudice along with a limited opportunity to amend the complaint within twenty days only became final and binding when no amended pleadings were filed within the time period allowed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to ref_case_legalBertLarge_sum_llama_holdings_without_hint_small/case_231_summary_holding.txt\n",
      "Time elapsed: 66.58955407142639\n",
      "Processing:  case_235_summary.txt\n",
      "holding that a claim for interest on a money judgment did not accrue until the judgment was rendered which was the date on which the trial court issued its order\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to ref_case_legalBertLarge_sum_llama_holdings_without_hint_small/case_235_summary_holding.txt\n",
      "Time elapsed: 103.6399302482605\n",
      "Processing:  case_242_summary.txt\n",
      "holding that a court may not rewrite the insurance contract under the guise of judicial interpretation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to ref_case_legalBertLarge_sum_llama_holdings_without_hint_small/case_242_summary_holding.txt\n",
      "Time elapsed: 58.29112195968628\n",
      "Processing:  case_249_summary.txt\n",
      "recognizing that intentional furnishing of false information of a material nature is breach of cooperation clause and discussing misrepresentations provided during investigation and trial of claim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to ref_case_legalBertLarge_sum_llama_holdings_without_hint_small/case_249_summary_holding.txt\n",
      "Time elapsed: 39.222641468048096\n",
      "Processing:  case_278_summary.txt\n",
      "holding that fixing tickets by passing them to the inactive files without requiring the defendants to appear in court and over the objections of the issuing officer constituted moral turpitude\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to ref_case_legalBertLarge_sum_llama_holdings_without_hint_small/case_278_summary_holding.txt\n",
      "Time elapsed: 80.85450530052185\n",
      "Processing:  case_284_summary.txt\n",
      "holding similarly where appellant claimed that she was entitled to recover attorney fees under nrs 18010 even though nrcp 68f and nrs 171154 foreclosed such a recovery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to ref_case_legalBertLarge_sum_llama_holdings_without_hint_small/case_284_summary_holding.txt\n",
      "Time elapsed: 169.42029929161072\n",
      "Processing:  case_289_summary.txt\n",
      "holding that juveniles may waive constitutional rights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to ref_case_legalBertLarge_sum_llama_holdings_without_hint_small/case_289_summary_holding.txt\n",
      "Time elapsed: 45.93712663650513\n",
      "Processing:  case_316_summary.txt\n",
      "holding that article i section 19 of the missouri constitution requires that once a witness claims the privilege against selfinerimination afforded by that provision a rebuttable presumption arises that the witness answer might tend to incriminate him\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to ref_case_legalBertLarge_sum_llama_holdings_without_hint_small/case_316_summary_holding.txt\n",
      "Time elapsed: 60.90019989013672\n",
      "Processing:  case_335_summary.txt\n",
      "recognizing if truth about paternity can be discovered and equity does not demand otherwise presumption of legitimacy should not be used to perpetuate a falsehood\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to ref_case_legalBertLarge_sum_llama_holdings_without_hint_small/case_335_summary_holding.txt\n",
      "Time elapsed: 68.29979920387268\n",
      "Processing:  case_352_summary.txt\n",
      "holding fjraud on the part of the insured in the procurement of the policy  is sufficient to defeat a recovery in an action on such policy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to ref_case_legalBertLarge_sum_llama_holdings_without_hint_small/case_352_summary_holding.txt\n",
      "Time elapsed: 54.50455331802368\n",
      "Processing:  case_79_summary.txt\n",
      "holding that appeals are taken from judgments and not from opinions let alone dicta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to ref_case_legalBertLarge_sum_llama_holdings_without_hint_small/case_79_summary_holding.txt\n",
      "Time elapsed: 54.875834226608276\n"
     ]
    }
   ],
   "source": [
    "input_folder = 'ref_case_legalBertLarge_summaries_small'\n",
    "output_folder = 'ref_case_legalBertLarge_sum_llama_holdings_without_hint_small'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "processed_files_df = None\n",
    "# Define the filename for the processed files DataFrame\n",
    "processed_files_csv = 'processed_files_legalBertLarge_sum_llama_for_holdings_without_hint_small.csv'\n",
    "\n",
    "for i in range(len(os.listdir(input_folder))):\n",
    "    start = time.time()\n",
    "    batch_process_holdings(1,processed_files_csv)\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed:\", end - start)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c84f64-5703-4e37-986f-02117fe58715",
   "metadata": {},
   "source": [
    "# Bulk Holding Generation with Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c103b4-fc0c-4d53-8717-d5af7863bb1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beb0c3a-bca7-4af6-94dc-7f7eac8b84ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03374086-689f-4422-90b2-6841e2ba6b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18e80e-e7b1-4232-a5ec-6a8343d2fd52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc6fd34-a965-49ca-a39d-0d7b1c013391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea29dfb-9be9-4cc7-a330-279514c5f85a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49beacdb-a8b2-4c01-aa05-cae6ca42df13",
   "metadata": {},
   "source": [
    "# Longformer Bulk Summarization\n",
    "## currently does not chunk text larger than context window\n",
    "## Will modify in the future if this proves to be a viable route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "329a8437-1f1a-41d2-a7a6-6fee46ac63c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LEDForConditionalGeneration, LEDTokenizer, pipeline, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d92a848-230a-425a-8d68-d8de180e8a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LEDForConditionalGeneration.from_pretrained('allenai/led-base-16384')\n",
    "tokenizer = LEDTokenizer.from_pretrained(\"allenai/led-base-16384\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80fc82b0-6a28-4081-9cf7-10e00a7d597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LED_model_folder = \"./LED\"\n",
    "# os.makedirs(LED_model_folder, exist_ok=True)\n",
    "# model.save_pretrained(LED_model_folder)\n",
    "# tokenizer.save_pretrained(LED_model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53c75739-b742-43a9-ba7f-15c6aca02b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LED_model_folder = \"./LED\"\n",
    "# tokenizer = LEDForConditionalGeneration.from_pretrained(LED_model_folder)\n",
    "# model = LEDTokenizer.from_pretrained(LED_model_folder)\n",
    "LED_summarizer_pipeline = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=0)  # 'device=0' to use the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "342ff9bf-0fd5-4960-8937-bcf7e0e98b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_a_batch_of_case_documents(batch_size, processed_files_csv):\n",
    " \n",
    "    # Check if the CSV file exists and load it, otherwise create an empty DataFrame\n",
    "    if os.path.exists(processed_files_csv):\n",
    "        processed_files_df = pd.read_csv(processed_files_csv)\n",
    "    else:\n",
    "        processed_files_df = pd.DataFrame(columns=[\"file_name\"])\n",
    "    \n",
    "    # Counter for the number of processed files in the batch\n",
    "    processed_count = 0\n",
    "\n",
    "    # Retrieve all filenames, sorted to ensure consistency across runs\n",
    "    filenames = sorted(os.listdir(input_folder))\n",
    "    \n",
    "    # Start from where the last entry left off\n",
    "    last_processed_index = 0\n",
    "    if not processed_files_df.empty:\n",
    "        last_filename = processed_files_df['file_name'].iloc[-1]\n",
    "        last_processed_index = filenames.index(last_filename) + 1\n",
    "\n",
    "    # Process files starting from the last processed one\n",
    "    for filename in filenames[last_processed_index:]:\n",
    "        if processed_count >= batch_size:\n",
    "            break\n",
    "        print(\"Processing: \", filename)\n",
    "        if filename.endswith(\".json\"):\n",
    "            if filename in processed_files_df['file_name'].values:\n",
    "                    print(f\"File {filename} has already been processed. Skipping.\")\n",
    "                    continue\n",
    "            else:\n",
    "                summarize_a_case_document(filename)\n",
    "                processed_count += 1  # Increment the processed files counter\n",
    "\n",
    "                # Add the file to the processed DataFrame\n",
    "                new_row = {\"file_name\": filename}\n",
    "                processed_files_df = pd.concat([processed_files_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "                processed_files_df.to_csv(processed_files_csv, index=False)\n",
    "                \n",
    "def summarize_a_case_document(filename):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    opinion = load_and_extract_data(file_path)\n",
    "    opinion = str(opinion)\n",
    "    \n",
    "    # chunk_word_count = 1000  # for instance, around 1000 words per chunk\n",
    "    # overlap_word_count = 200  # for instance, overlap of 200 words\n",
    "    \n",
    "    # chunks = chunk_text_with_overlap(opinion, chunk_word_count, overlap_word_count)\n",
    "\n",
    "    summary = LED_summarizer_pipeline(opinion, min_length=30, max_length=1000)\n",
    "    summary_text = summary[0]['summary_text']\n",
    "\n",
    "    # Save the summary to a text file\n",
    "    save_summary_to_text(summary_text, output_folder, file_path, condensed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d55caf4-e87b-4fbb-9e42-ece03dcf1923",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'ref_case_small'\n",
    "output_folder = 'ref_case_LED_summaries_small'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "processed_files_df = None\n",
    "\n",
    "# Define the filename for the processed files DataFrame\n",
    "processed_files_csv = 'processed_files_for_LED_summarizing_small.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7df10321-6a59-4378-a84d-fe3a32be182f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  case_104.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_104_summary.txt\n",
      "Time elapsed: 7.562493801116943\n",
      "Processing:  case_116.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_116_summary.txt\n",
      "Time elapsed: 7.652428388595581\n",
      "Processing:  case_135.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_135_summary.txt\n",
      "Time elapsed: 6.961967468261719\n",
      "Processing:  case_146.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_146_summary.txt\n",
      "Time elapsed: 6.355541944503784\n",
      "Processing:  case_175.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_175_summary.txt\n",
      "Time elapsed: 6.683484077453613\n",
      "Processing:  case_225.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_225_summary.txt\n",
      "Time elapsed: 7.277122259140015\n",
      "Processing:  case_231.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_231_summary.txt\n",
      "Time elapsed: 6.84537148475647\n",
      "Processing:  case_235.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_235_summary.txt\n",
      "Time elapsed: 7.338288307189941\n",
      "Processing:  case_242.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_242_summary.txt\n",
      "Time elapsed: 7.209223985671997\n",
      "Processing:  case_249.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_249_summary.txt\n",
      "Time elapsed: 7.26111626625061\n",
      "Processing:  case_278.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_278_summary.txt\n",
      "Time elapsed: 7.177913665771484\n",
      "Processing:  case_284.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_284_summary.txt\n",
      "Time elapsed: 2.1981449127197266\n",
      "Processing:  case_289.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_289_summary.txt\n",
      "Time elapsed: 6.974234580993652\n",
      "Processing:  case_316.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_316_summary.txt\n",
      "Time elapsed: 7.456243276596069\n",
      "Processing:  case_335.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_335_summary.txt\n",
      "Time elapsed: 6.600247383117676\n",
      "Processing:  case_352.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_352_summary.txt\n",
      "Time elapsed: 7.210155963897705\n",
      "Processing:  case_79.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_79_summary.txt\n",
      "Time elapsed: 7.394057512283325\n",
      "Time elapsed: 0.004387378692626953\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(os.listdir(input_folder))):\n",
    "    start = time.time()\n",
    "    summarize_a_batch_of_case_documents(1, processed_files_csv)\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed:\", end - start)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956b84eb-7b2a-4abd-baf6-7361c11ab4e3",
   "metadata": {},
   "source": [
    "# Long T5 Bulk Summarization\n",
    "## Currently does not chunk text larger than context window\n",
    "## Will modify in the future if this proves to be a viable route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80965a0c-dae9-4f3f-86cf-e06e0f3de8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8900a3edd8b94ad4b44b69d26de83024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()lve/main/config.json:   0%|          | 0.00/853 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64a0b6bc33d4d55a98fad3d651771a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b8be02c5f348b195ecb8f506978307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()okenizer_config.json:   0%|          | 0.00/2.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c755d45be2743b49771f90a05f3e115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c649573989421f9cb13b58c7ce8c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, LongT5ForConditionalGeneration, pipeline\n",
    "model = (\n",
    "    LongT5ForConditionalGeneration.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")\n",
    "    .to(\"cuda\")\n",
    "    .half()\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f62e00cd-1354-4652-9ee8-1524eb1d0cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Long_T5_summarizer_pipeline = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=0)  # 'device=0' to use the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a3b970be-6a12-4319-9569-716f73ec633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_a_batch_of_case_documents(batch_size, processed_files_csv, pipeline):\n",
    " \n",
    "    # Check if the CSV file exists and load it, otherwise create an empty DataFrame\n",
    "    if os.path.exists(processed_files_csv):\n",
    "        processed_files_df = pd.read_csv(processed_files_csv)\n",
    "    else:\n",
    "        processed_files_df = pd.DataFrame(columns=[\"file_name\"])\n",
    "    \n",
    "    # Counter for the number of processed files in the batch\n",
    "    processed_count = 0\n",
    "\n",
    "    # Retrieve all filenames, sorted to ensure consistency across runs\n",
    "    filenames = sorted(os.listdir(input_folder))\n",
    "    \n",
    "    # Start from where the last entry left off\n",
    "    last_processed_index = 0\n",
    "    if not processed_files_df.empty:\n",
    "        last_filename = processed_files_df['file_name'].iloc[-1]\n",
    "        last_processed_index = filenames.index(last_filename) + 1\n",
    "\n",
    "    # Process files starting from the last processed one\n",
    "    for filename in filenames[last_processed_index:]:\n",
    "        if processed_count >= batch_size:\n",
    "            break\n",
    "        print(\"Processing: \", filename)\n",
    "        if filename.endswith(\".json\"):\n",
    "            if filename in processed_files_df['file_name'].values:\n",
    "                    print(f\"File {filename} has already been processed. Skipping.\")\n",
    "                    continue\n",
    "            else:\n",
    "                summarize_a_case_document(filename, pipeline)\n",
    "                processed_count += 1  # Increment the processed files counter\n",
    "\n",
    "                # Add the file to the processed DataFrame\n",
    "                new_row = {\"file_name\": filename}\n",
    "                processed_files_df = pd.concat([processed_files_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "                processed_files_df.to_csv(processed_files_csv, index=False)\n",
    "                \n",
    "def summarize_a_case_document(filename, pipeline):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    opinion = load_and_extract_data(file_path)\n",
    "    opinion = str(opinion)\n",
    "    \n",
    "    # chunk_word_count = 1000  # for instance, around 1000 words per chunk\n",
    "    # overlap_word_count = 200  # for instance, overlap of 200 words\n",
    "    \n",
    "    # chunks = chunk_text_with_overlap(opinion, chunk_word_count, overlap_word_count)\n",
    "\n",
    "    summary = pipeline(opinion, min_length=30, max_length=1000)\n",
    "    summary_text = summary[0]['summary_text']\n",
    "\n",
    "    # Save the summary to a text file\n",
    "    save_summary_to_text(summary_text, output_folder, file_path, condensed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "45c3b20b-46b7-4819-9df9-110bb8e952dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'ref_case_small'\n",
    "output_folder = 'ref_case_Long_T5_summaries_small'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "processed_files_df = None\n",
    "\n",
    "# Define the filename for the processed files DataFrame\n",
    "processed_files_csv = 'processed_files_for_Long_T5_summarizing_small.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22b6213e-8442-4e01-9c0b-b644b18d0bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  case_103.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:863: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_103_summary.txt\n",
      "Time elapsed: 9.881898880004883\n",
      "Processing:  case_104.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:863: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_104_summary.txt\n",
      "Time elapsed: 7.390232563018799\n",
      "Processing:  case_116.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:863: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_116_summary.txt\n",
      "Time elapsed: 6.77020788192749\n",
      "Processing:  case_135.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:863: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_135_summary.txt\n",
      "Time elapsed: 3.730167865753174\n",
      "Processing:  case_146.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:863: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_146_summary.txt\n",
      "Time elapsed: 58.532248735427856\n",
      "Processing:  case_175.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:863: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_175_summary.txt\n",
      "Time elapsed: 59.602532625198364\n",
      "Processing:  case_225.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:863: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_225_summary.txt\n",
      "Time elapsed: 6.0869269371032715\n",
      "Processing:  case_231.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:863: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_231_summary.txt\n",
      "Time elapsed: 61.00665855407715\n",
      "Processing:  case_235.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:863: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_235_summary.txt\n",
      "Time elapsed: 5.824497938156128\n",
      "Processing:  case_242.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:863: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_242_summary.txt\n",
      "Time elapsed: 60.64083981513977\n",
      "Processing:  case_249.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:863: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_249_summary.txt\n",
      "Time elapsed: 7.613674163818359\n",
      "Processing:  case_278.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:863: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_278_summary.txt\n",
      "Time elapsed: 5.797230005264282\n",
      "Processing:  case_284.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:863: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_284_summary.txt\n",
      "Time elapsed: 4.169192552566528\n",
      "Processing:  case_289.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:863: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_289_summary.txt\n",
      "Time elapsed: 4.906538963317871\n",
      "Processing:  case_316.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:863: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_316_summary.txt\n",
      "Time elapsed: 7.803145170211792\n",
      "Processing:  case_335.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:863: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_335_summary.txt\n",
      "Time elapsed: 5.496894121170044\n",
      "Processing:  case_352.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:863: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_352_summary.txt\n",
      "Time elapsed: 7.082246780395508\n",
      "Processing:  case_79.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:863: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary successfully written to case_79_summary.txt\n",
      "Time elapsed: 57.95717000961304\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(os.listdir(input_folder))):\n",
    "    start = time.time()\n",
    "    summarize_a_batch_of_case_documents(1, processed_files_csv, Long_T5_summarizer_pipeline)\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed:\", end - start)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f7ffb0-a8f4-4a0f-9121-0e89b7b0831e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
